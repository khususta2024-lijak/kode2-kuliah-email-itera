{"cells":[{"cell_type":"markdown","metadata":{"id":"1wRYSxRyLFGD"},"source":["# Petunjuk\n","\n","Buatkan komparasi kondisi awal dan sesudah modifikasi sesuai dengan tugas yang di berikan, lalu buatkan code baru dengan modifikasi sesuaian dengan soal yang diberikan. kunci analisis akan menjadi tes interview setelah berhasil mendapatkan hasil komparasi. setiap poin akan di jumlahkan dan menjadi nilai tugas individu."]},{"cell_type":"markdown","metadata":{"id":"RrO8NchwLFGF"},"source":["# Soal 1 [20 poin]\n","\n","Tingkatkan performa GRU dengan mengubah fungsi aktivasi dan mengintegrasikan fitur embedding dari pre-trained model seperti Word2Vec.\n","\n","Tugas:\n","\n","1. Ubah fungsi aktivasi dari tanh menjadi ReLU.\n","2. Gunakan embedding dari Word2Vec untuk merepresentasikan data input.\n","\n","Kunci Analisis: Lakukan perbandingan performa model sebelum dan sesudah modifikasi. Berikan comment anda terhadap akurasinya, di bandingkan yang ada di modul."]},{"cell_type":"markdown","metadata":{"id":"xKurpKP-LFGG"},"source":["Bahan untuk melakukan eksperimen:"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":228},"id":"dHSfoBdnLFGG","executionInfo":{"status":"error","timestamp":1731998887125,"user_tz":-420,"elapsed":346,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}},"outputId":"3047c5a1-d0fc-4681-98bb-98bd541af5c0"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-13e3d62524ad>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence_to_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Contoh label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part."]}],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, GRU, Dense\n","from gensim.models import Word2Vec\n","\n","# Data dummy (ganti dengan data sebenarnya dari modul)\n","sentences = [\n","    \"ini adalah contoh kalimat pertama\".split(),\n","    \"chatbot membutuhkan evaluasi dan metrik\".split(),\n","    \"performa model harus dibandingkan\".split(),\n","]\n","\n","# 1. Word2Vec Embedding\n","# Membuat embedding Word2Vec\n","word2vec = Word2Vec(sentences, vector_size=50, min_count=1, workers=4)\n","vocab_size = len(word2vec.wv)\n","\n","# Membuat word-index mapping\n","word_index = {word: i for i, word in enumerate(word2vec.wv.index_to_key)}\n","\n","# Mengubah data ke bentuk indeks\n","def sentence_to_indices(sentence):\n","    return [word_index[word] for word in sentence]\n","\n","data = np.array([sentence_to_indices(sentence) for sentence in sentences])\n","labels = np.array([0, 1, 1])  # Contoh label\n","\n","# Padding sequences\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","max_length = max(len(seq) for seq in data)\n","data_padded = pad_sequences(data, maxlen=max_length, padding=\"post\")\n","\n","# 2. Membuat Model GRU dengan tanh (Baseline)\n","model_tanh = Sequential([\n","    Embedding(input_dim=vocab_size, output_dim=50, weights=[word2vec.wv.vectors], trainable=False),\n","    GRU(128, activation='tanh', return_sequences=False),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","model_tanh.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Training Baseline\n","print(\"Training Model dengan tanh\")\n","model_tanh.fit(data_padded, labels, epochs=10, batch_size=2, verbose=1)\n","\n","# Evaluasi Baseline\n","loss_tanh, acc_tanh = model_tanh.evaluate(data_padded, labels, verbose=0)\n","print(f\"Accuracy dengan tanh: {acc_tanh:.4f}\")\n","\n","# 3. Modifikasi GRU dengan ReLU\n","model_relu = Sequential([\n","    Embedding(input_dim=vocab_size, output_dim=50, weights=[word2vec.wv.vectors], trainable=False),\n","    GRU(128, activation='relu', return_sequences=False),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","model_relu.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Training Model dengan ReLU\n","print(\"Training Model dengan ReLU\")\n","model_relu.fit(data_padded, labels, epochs=10, batch_size=2, verbose=1)\n","\n","# Evaluasi dengan ReLU\n","loss_relu, acc_relu = model_relu.evaluate(data_padded, labels, verbose=0)\n","print(f\"Accuracy dengan ReLU: {acc_relu:.4f}\")\n","\n","# 4. Analisis\n","print(\"\\n--- Analisis ---\")\n","print(f\"Accuracy Baseline (tanh): {acc_tanh:.4f}\")\n","print(f\"Accuracy dengan ReLU: {acc_relu:.4f}\")\n","if acc_relu > acc_tanh:\n","    print(\"Model dengan ReLU memiliki performa lebih baik.\")\n","else:\n","    print(\"Model baseline (tanh) memiliki performa lebih baik.\")\n"]},{"cell_type":"markdown","metadata":{"id":"JfTiPjxVLFGI"},"source":["# Soal 2 [40 poin]\n","\n","Gabungkan Bidirectional LSTM dengan GRU dalam satu arsitektur hibrid.\n","\n","Tugas:\n","\n","1. Bangun model yang memiliki kombinasi layer Bidirectional LSTM dan GRU, **buatkan lebih dari satu lapisan**\n","2. Eksperimen dengan berbagai ukuran hidden layer (64, 128, dan 256).\n","\n","Kunci Analisis: Evaluasi pengaruh ukuran hidden layer terhadap akurasi prediksi. lihat sesudah dilakukan eksperimen dengan yang ada di modul."]},{"cell_type":"markdown","metadata":{"id":"nBI5XcMhLFGJ"},"source":["Bahan untuk Melakukan Eksperimen:"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9EU2IJwoLFGJ","executionInfo":{"status":"ok","timestamp":1731998929364,"user_tz":-420,"elapsed":26209,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}},"outputId":"38986848-2a70-4473-a07d-36ddd1500c12"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training model dengan hidden size: 64\n","Epoch 1/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.6111 - loss: 0.6921\n","Epoch 2/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6111 - loss: 0.6868 \n","Epoch 3/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6111 - loss: 0.6811\n","Epoch 4/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6111 - loss: 0.6753\n","Epoch 5/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6111 - loss: 0.6688\n","Epoch 6/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6111 - loss: 0.6623\n","Epoch 7/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6111 - loss: 0.6550\n","Epoch 8/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6111 - loss: 0.6471\n","Epoch 9/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6111 - loss: 0.6398\n","Epoch 10/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6111 - loss: 0.6342\n","Accuracy dengan hidden size 64: 0.6667\n","\n","Training model dengan hidden size: 128\n","Epoch 1/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.3889 - loss: 0.6952\n","Epoch 2/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.6880\n","Epoch 3/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6111 - loss: 0.6800\n","Epoch 4/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6111 - loss: 0.6706\n","Epoch 5/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6111 - loss: 0.6593 \n","Epoch 6/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6111 - loss: 0.6461\n","Epoch 7/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6111 - loss: 0.6325 \n","Epoch 8/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6111 - loss: 0.6203\n","Epoch 9/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6111 - loss: 0.6145 \n","Epoch 10/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7778 - loss: 0.4978\n","Accuracy dengan hidden size 128: 0.6667\n","\n","Training model dengan hidden size: 256\n","Epoch 1/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 0.6922\n","Epoch 2/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6111 - loss: 0.6795\n","Epoch 3/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6111 - loss: 0.6638\n","Epoch 4/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.6111 - loss: 0.6415\n","Epoch 5/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6111 - loss: 0.6160\n","Epoch 6/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.7778 - loss: 0.5310 \n","Epoch 7/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.6111 - loss: 0.6201\n","Epoch 8/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7778 - loss: 0.4479 \n","Epoch 9/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7778 - loss: 0.4169\n","Epoch 10/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6111 - loss: 0.5399\n","Accuracy dengan hidden size 256: 0.6667\n","\n","--- Analisis ---\n","Hidden size 64: Accuracy = 0.6667\n","Hidden size 128: Accuracy = 0.6667\n","Hidden size 256: Accuracy = 0.6667\n","Model dengan hidden size terbaik adalah 64 dengan akurasi 0.6667\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, GRU, Dense\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from gensim.models import Word2Vec\n","\n","# Data dummy (ganti dengan data dari modul)\n","sentences = [\n","    \"model deep learning memerlukan eksperimen\".split(),\n","    \"bidirectional LSTM memproses data sekuensial\".split(),\n","    \"kombinasi dengan GRU meningkatkan performa\".split(),\n","]\n","\n","labels = np.array([0, 1, 1])  # Label dummy\n","\n","# 1. Word2Vec Embedding\n","word2vec = Word2Vec(sentences, vector_size=50, min_count=1, workers=4)\n","vocab_size = len(word2vec.wv)\n","\n","# Membuat word-index mapping\n","word_index = {word: i for i, word in enumerate(word2vec.wv.index_to_key)}\n","\n","# Mengubah data ke bentuk indeks\n","def sentence_to_indices(sentence):\n","    return [word_index[word] for word in sentence]\n","\n","data = np.array([sentence_to_indices(sentence) for sentence in sentences])\n","\n","# Padding sequences\n","max_length = max(len(seq) for seq in data)\n","data_padded = pad_sequences(data, maxlen=max_length, padding=\"post\")\n","\n","# 2. Model Hibrid: Bidirectional LSTM dan GRU\n","def build_hybrid_model(hidden_size):\n","    model = Sequential([\n","        Embedding(input_dim=vocab_size, output_dim=50, weights=[word2vec.wv.vectors], trainable=False),\n","        Bidirectional(LSTM(hidden_size, return_sequences=True)),\n","        GRU(hidden_size, return_sequences=False),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","# 3. Eksperimen dengan berbagai hidden layer size\n","hidden_sizes = [64, 128, 256]\n","results = {}\n","\n","for size in hidden_sizes:\n","    print(f\"\\nTraining model dengan hidden size: {size}\")\n","    model = build_hybrid_model(size)\n","    history = model.fit(data_padded, labels, epochs=10, batch_size=2, verbose=1)\n","    loss, accuracy = model.evaluate(data_padded, labels, verbose=0)\n","    results[size] = accuracy\n","    print(f\"Accuracy dengan hidden size {size}: {accuracy:.4f}\")\n","\n","# 4. Analisis\n","print(\"\\n--- Analisis ---\")\n","for size, acc in results.items():\n","    print(f\"Hidden size {size}: Accuracy = {acc:.4f}\")\n","\n","best_size = max(results, key=results.get)\n","print(f\"Model dengan hidden size terbaik adalah {best_size} dengan akurasi {results[best_size]:.4f}\")\n"]},{"cell_type":"markdown","source":["1. bangun model yang memiliki kombinasi layer bidirectional LSTM dan GRU, buatkan lebih dari satu lapisan\n","2. eksperimen dengan berbagai ukuran hidden layer (64, 128, dan 256)\n","\n","dan analisislah dengan kunci = evaluasi pengarus ukuran hidden layer terhadap akurasi prediksi. lihatlah sesudah dilakukan eksperimen dengan yang ada di modul\n","\n","jawablah tugas diatas dengan mengubah kode dibawah ini"],"metadata":{"id":"9_8zcLsELr4D"}},{"cell_type":"markdown","metadata":{"id":"-pqKtnriLFGJ"},"source":["# Soal 3 [40 poin]\n","\n","Bagaimana arsitektur Bidirectional memengaruhi pemahaman konteks dalam sekuens data?\n","\n","Tugas:\n","Eksperimen dengan model Bidirectional LSTM dengan dan tanpa pre-trained embedding (seperti GloVe).\n","\n","Kunci Analisis: Apakah penggunaan pre-trained embedding memberikan keuntungan signifikan dibandingkan tanpa embedding? Jelaskan hasil eksperimen Anda (berikan comment atau markdown)."]},{"cell_type":"markdown","metadata":{"id":"0AKLiDdoLFGK"},"source":["Bahan untuk melakukan eksperimen:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g4L-Rf8ELFGK"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# Data dummy (ganti dengan data dari modul)\n","sentences = [\n","    \"pemahaman konteks dalam data sekuensial sangat penting\",\n","    \"bidirectional LSTM membaca data dari dua arah\",\n","    \"penggunaan embedding pretrained dapat meningkatkan akurasi\",\n","]\n","\n","labels = np.array([0, 1, 1])  # Label dummy\n","\n","# 1. Tokenisasi dan Padding\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","data = tokenizer.texts_to_sequences(sentences)\n","max_length = max(len(seq) for seq in data)\n","data_padded = pad_sequences(data, maxlen=max_length, padding=\"post\")\n","\n","# 2. Memuat GloVe Pre-trained Embedding\n","embedding_index = {}\n","glove_file = \"glove.6B.50d.txt\"  # Pastikan file ini tersedia\n","with open(glove_file, encoding=\"utf-8\") as f:\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype=\"float32\")\n","        embedding_index[word] = coefs\n","\n","# Membuat matriks embedding\n","embedding_dim = 50\n","vocab_size = len(word_index) + 1\n","embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","for word, i in word_index.items():\n","    embedding_vector = embedding_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","# 3. Model dengan Pre-trained GloVe\n","def build_model_with_glove():\n","    model = Sequential([\n","        Embedding(input_dim=vocab_size, output_dim=embedding_dim,\n","                  weights=[embedding_matrix], trainable=False),\n","        Bidirectional(LSTM(128, return_sequences=False)),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","# 4. Model tanpa Pre-trained Embedding\n","def build_model_without_glove():\n","    model = Sequential([\n","        Embedding(input_dim=vocab_size, output_dim=embedding_dim, trainable=True),\n","        Bidirectional(LSTM(128, return_sequences=False)),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","# Training dan evaluasi model\n","print(\"\\n--- Model dengan Pre-trained GloVe ---\")\n","model_with_glove = build_model_with_glove()\n","model_with_glove.fit(data_padded, labels, epochs=10, batch_size=2, verbose=1)\n","loss_with_glove, acc_with_glove = model_with_glove.evaluate(data_padded, labels, verbose=0)\n","print(f\"Accuracy dengan pre-trained embedding GloVe: {acc_with_glove:.4f}\")\n","\n","print(\"\\n--- Model tanpa Pre-trained Embedding ---\")\n","model_without_glove = build_model_without_glove()\n","model_without_glove.fit(data_padded, labels, epochs=10, batch_size=2, verbose=1)\n","loss_without_glove, acc_without_glove = model_without_glove.evaluate(data_padded, labels, verbose=0)\n","print(f\"Accuracy tanpa pre-trained embedding: {acc_without_glove:.4f}\")\n","\n","# Analisis\n","print(\"\\n--- Analisis ---\")\n","print(f\"Accuracy dengan pre-trained GloVe: {acc_with_glove:.4f}\")\n","print(f\"Accuracy tanpa pre-trained embedding: {acc_without_glove:.4f}\")\n","if acc_with_glove > acc_without_glove:\n","    print(\"Penggunaan pre-trained GloVe memberikan keuntungan dalam akurasi.\")\n","else:\n","    print(\"Model tanpa pre-trained embedding memiliki performa serupa atau lebih baik.\")\n"]},{"cell_type":"markdown","metadata":{"id":"5r_ayBKgLFGK"},"source":["**Contoh Output**\n","\n","--- Model dengan Pre-trained GloVe ---\n","Accuracy dengan pre-trained embedding GloVe: 0.9000\n","\n","--- Model tanpa Pre-trained Embedding ---\n","Accuracy tanpa pre-trained embedding: 0.8500\n","\n","--- Analisis ---\n","Model dengan pre-trained GloVe memberikan akurasi lebih tinggi dibandingkan model tanpa pre-trained embedding.\n","Hal ini menunjukkan bahwa embedding pre-trained memiliki informasi semantik yang mempercepat proses belajar dan meningkatkan performa model.\n"]},{"cell_type":"code","source":["#nomor 1\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, GRU, Dense\n","from gensim.models import Word2Vec\n","from tensorflow.keras.preprocessing.sequence import pad_sequences # Import pad_sequences here\n","\n","# Data dummy (ganti dengan data sebenarnya dari modul)\n","sentences = [\n","    \"ini adalah contoh kalimat pertama\".split(),\n","    \"chatbot membutuhkan evaluasi dan metrik\".split(),\n","    \"performa model harus dibandingkan\".split(),\n"," ]\n","\n","# 1. Word2Vec Embedding\n","# Membuat embedding Word2Vec\n","word2vec = Word2Vec (sentences, vector_size=50, min_count=1, workers=4)\n","vocab_size = len(word2vec.wv)\n","\n","#Membuat word-index mapping\n","word_index = {word: i for i, word in enumerate (word2vec.wv.index_to_key)}\n","\n","# Mengubah data ke bentuk indeks\n","def sentence_to_indices(sentence):\n","  return [word_index[word] for word in sentence]\n","\n","# Convert sentences to indices\n","data = [sentence_to_indices(sentence) for sentence in sentences]\n","\n","# Padding sequences to ensure uniform length\n","max_length = max(len(seq) for seq in data)  # Calculate max length\n","data_padded = pad_sequences(data, maxlen=max_length, padding=\"post\") # Pad the sequences\n","\n","# Now convert to NumPy array\n","data = np.array(data_padded) # Assign the padded data to 'data'\n","\n","labels = np.array([0, 1, 1]) # Contoh Label\n","\n","\n","\n","# 2. Membuat Model GRU dengan tanh (Baseline)\n","model_tanh = Sequential([\n","    Embedding(input_dim=vocab_size, output_dim=50, weights=[word2vec.wv.vectors]),\n","    GRU(128, activation= 'tanh', return_sequences=False),\n","    Dense(1, activation='sigmoid')\n","])\n","model_tanh.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Training Baseline\n","print(\"Training Model dengan tanh\")\n","model_tanh.fit(data_padded, labels, epochs=10, batch_size=2, verbose=1)\n","\n","#Evaluasi Baseline\n","loss_tanh, acc_tanh = model_tanh.evaluate(data_padded, labels, verbose=0)\n","print (f\"Accuracy dengan tanh: {acc_tanh:.4f}\")\n","\n","# 3. Modifikasi GRU dengan ReLU\n","model_relu = Sequential([\n","    Embedding(input_dim=vocab_size, output_dim=50, weights=[word2vec.wv.vectors]),\n","    GRU(128, activation='relu', return_sequences=False),\n","    Dense(1, activation='sigmoid')\n","])\n","model_relu.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Training Model dengan ReLU\n","print(\"Training Model dengan ReLU\")\n","model_relu.fit(data_padded, labels, epochs=10, batch_size=2, verbose=1)\n","\n","#Evaluasi dengan ReLU\n","loss_relu, acc_relu = model_relu.evaluate(data_padded, labels, verbose=0)\n","print (f\"Accuracy dengan ReLU: {acc_relu:.4f}\")\n","\n","#"],"metadata":{"id":"1B9jhJ49Qnyx"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}