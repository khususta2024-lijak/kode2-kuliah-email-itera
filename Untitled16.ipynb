{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPYjSwXEKzcc3rkUaeAAURw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["pip install pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OHPcqAKIucuD","executionInfo":{"status":"ok","timestamp":1715650351275,"user_tz":-420,"elapsed":51505,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}},"outputId":"ef8aad37-71a9-4fa7-fea8-539fb852959e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=72ede8684f55616bd0b2b84bb8e67f467f964526179f793b4d77f959080c56e6\n","  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.1\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"la5loU4Qt-kN","executionInfo":{"status":"ok","timestamp":1715650372898,"user_tz":-420,"elapsed":2142,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"outputs":[],"source":["import pyspark\n","from pyspark.sql import *\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from datetime import datetime"]},{"cell_type":"code","source":["#read from dbfs\n","df = spark.read.load(\"dbfs:/BTC-USD.csv\",\n","                      format=\"csv\",\n","                      sep=\",\",\n","                      inferSchema=\"true\",\n","                      header=\"true\");"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"I_uk6Y0SuBou","executionInfo":{"status":"error","timestamp":1715650785991,"user_tz":-420,"elapsed":329,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}},"outputId":"47116baf-8386-44ad-ab03-b3cf412f57b0"},"execution_count":9,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'spark' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-9b52d620b034>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#read from dbfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m df = spark.read.load(\"dbfs:/BTC-USD.csv\",\n\u001b[0m\u001b[1;32m      3\u001b[0m                       \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                       \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                       \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"true\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"]}]},{"cell_type":"code","source":["df.createOrReplaceTempView('df_table')\n","train_set = spark.sql(\"SELECT Date, Close from df_table where Date < '20190917'\")\n","test_set = spark.sql(\"SELECT Date, Close from df_table where Date >= '20190917'\")"],"metadata":{"id":"Q1AsH5d6vw9K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.stat import Correlation\n","from pyspark.ml.feature import VectorAssembler\n","\n","def show_matrix(matrix):\n","    \"\"\"\n","    function to print a matrix on screen\n","    \"\"\"\n","    print(matrix.collect()[0][matrix.columns[0]].toArray())\n","    print()\n","\n","vector_col = \"features\"\n","assembler = VectorAssembler(inputCols=[\"Open\", \"Close\", \"High\", \"Low\", \"Volume\"], outputCol=vector_col)\n","df_vector = assembler.transform(df).select(vector_col)\n","\n","matrix_pearson = Correlation.corr(df_vector, vector_col)    #pearson is default\n","matrix_spearman = Correlation.corr(df_vector, vector_col, \"spearman\")\n","show_matrix(matrix_pearson)\n","show_matrix(matrix_spearman)"],"metadata":{"id":"Ua7MuIG6v1qM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import mean as _mean, stddev as _stddev, col, udf\n","from pyspark.sql.types import FloatType, StructType, ArrayType\n","\n","\n","train_set_stats = train_set.select(\n","    _mean(col('Close')).alias('mean'),\n","    _stddev(col('Close')).alias('std')\n",").collect()\n","mean = train_set_stats[0]['mean']    #mean of close prices\n","std = train_set_stats[0]['std']    #standard deviation of close prices\n","\n","\n","@udf(returnType=FloatType())\n","def tanh_estimator(x):\n","    \"\"\"\n","    user defined function, applies tanh estimator's formula to a feature value x\n","    \"\"\"\n","    return (float)(0.5 * (np.tanh(0.01*(x-mean)/std) + 1))\n","\n","def scale_transform(df):\n","    \"\"\"\n","    tranforms a dataframe applying tanh estimator udf\n","    \"\"\"\n","    return df.select(\"Date\", tanh_estimator(\"Close\").alias(\"Close\"))\n","\n","scaled_train_set = scale_transform(train_set)\n","scaled_test_set = scale_transform(test_set)"],"metadata":{"id":"S5qB3LaYv4BB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.window import Window\n","\n","def slide_window(df, window_size):\n","    \"\"\"\n","    Returns two new dataframes:\n","    X - obtained sliding a window of given size (=#window_size) on the original dataframe, aggregating #window_size close prices on the same row\n","    y - for each row of X, y contains a row with the (single) price of the day after last day contained in X\n","    \"\"\"\n","\n","    w = Window.orderBy(\"Date\")\n","    indexed_df = df.withColumn(\"Index\", row_number().over(w)).select(\"Index\", \"Close\")    #adding index to be able to loop following order and create windows\n","\n","    schema = StructType([StructField(\"Close\", ArrayType(FloatType()), False)])   #schema for X (array of floats)\n","\n","    X = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema )\n","    y = spark.createDataFrame(spark.sparkContext.emptyRDD(), FloatType())\n","\n","    length = indexed_df.count()\n","    for i in range(window_size+1, length+1):\n","        new_df = indexed_df.where(col(\"Index\").between(i-window_size, i-1)).select(\"Close\")    #select the window\n","        new_row = new_df.agg(collect_list(\"Close\").alias(\"Close\"))    #create new X's row with all prices from window\n","        X = X.union(new_row)\n","        new_row = indexed_df.where(col(\"Index\") == i).select(\"Close\")    #create new Y's row with price of the day after last day contained in X\n","        y = y.union(new_row)\n","\n","    return X, y"],"metadata":{"id":"J6qtruvOv6LG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["window = 30    #window size\n","\n","X_train, y_train = slide_window(scaled_train_set, window)    #slide window on train set\n","X_test, y_test = slide_window(scaled_test_set, window)    #slide window on test set"],"metadata":{"id":"ukK-FbVWv8fJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def merge_X_y(X, y):\n","    \"\"\"\n","    merges two dataframes column-wise\n","    \"\"\"\n","    schema = StructType(X.schema.fields + y.schema.fields)\n","    X_y = X.rdd.zip(y.rdd).map(lambda x: x[0]+x[1])\n","    return spark.createDataFrame(X_y, schema)\n","\n","X_y_train = merge_X_y(X_train, y_train)\n","X_y_test = merge_X_y(X_test, y_test)"],"metadata":{"id":"5zqAT4Etv-Uy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors, VectorUDT\n","from pyspark.sql.functions import udf\n","from pyspark import StorageLevel\n","\n","list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())    #converts list of prices to features vector\n","\n","def assemble_window(X_y):\n","    \"\"\"\n","    applies list_to_vector_udf to given dataframe\n","    \"\"\"\n","    return X_y.select(list_to_vector_udf(X_y[\"Close\"]).alias(\"features\"), X_y[\"value\"].alias(\"label\"))\n","\n","X_y_train_vec = assemble_window(X_y_train)\n","X_y_test_vec = assemble_window(X_y_test)"],"metadata":{"id":"UOiTlZGCv_D-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n","from pyspark.ml.evaluation import RegressionEvaluator\n","\n","def cross_validate(model, param_grid, df):\n","    \"\"\"\n","    Performs grid search on given model with given parameter grid, using given dataframe as train/validation data.\n","    Returns the validated model (ready to be used for predictions using best parameters found)\n","    \"\"\"\n","    evaluator = RegressionEvaluator(metricName=\"rmse\")\n","    cv = CrossValidator(estimator=model, estimatorParamMaps=param_grid, evaluator=evaluator)\n","    validated_model = cv.fit(df)\n","    return validated_model"],"metadata":{"id":"oGD3PYpLwBPu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.regression import LinearRegression\n","\n","evaluator = RegressionEvaluator(metricName=\"rmse\")\n","\n","lr = LinearRegression(standardization=False)    #avoid standardization as tanh estimator has been applied yet\n","param_grid = ParamGridBuilder().addGrid(lr.regParam, [0.33, 0.66]).addGrid(lr.elasticNetParam, [0.33, 0.5, 0.66]).build()    #parameters to be tuned\n","validated_lr_model = cross_validate(lr, param_grid, X_y_train_vec)\n","\n","#best parameters found\n","elasticNet = validated_lr_model.bestModel.getElasticNetParam()\n","reg = validated_lr_model.bestModel.getRegParam()\n","\n","print(\"ElasicNetParam of best model -> \", elasticNet)\n","print(\"RegParam of best model -> = \", reg)\n","\n","predictions = validated_lr_model.transform(X_y_test_vec)    #test on unseen data\n","RMSE = evaluator.evaluate(predictions)    #evaluate predictions using ROOT MEAN SQUARED ERROR\n","print(\"RMSE of best model on unseen data -> \", RMSE)"],"metadata":{"id":"UmJfOfKCwDkv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_predictions(predictions):\n","    \"\"\"\n","    plots two lines representing predicted and actual prices\n","    \"\"\"\n","    pandas_df = predictions.select('label', 'prediction').toPandas()\n","    plt.figure(figsize=(20, 7))\n","    plt.plot(range(len(pandas_df['label'].values)), pandas_df['label'].values, label = 'Actual Price', color = 'blue')\n","    plt.plot(range(len(pandas_df['prediction'].values)), pandas_df['prediction'].values, label = 'Predicted Price', color = 'red')\n","    plt.xticks(np.arange(100, pandas_df.shape[0], 200))\n","    plt.xlabel('Time')\n","    plt.ylabel('Price (scaled)')\n","    plt.legend()\n","    plt.show()\n","\n","plot_predictions(predictions)    #plot linear regression's predictions"],"metadata":{"id":"1H06_nB5wFtz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.regression import GBTRegressor\n","\n","gbt_r = GBTRegressor()\n","param_grid = ParamGridBuilder().addGrid(gbt_r.maxDepth, [4, 8, 12]).addGrid(gbt_r.featureSubsetStrategy, ['0.33', '0.66']).build()    #parameters to be tuned\n","validated_gbt_model = cross_validate(gbt_r, param_grid, X_y_train_vec)\n","\n","#best parameters found\n","max_depth = validated_gbt_model.bestModel.getMaxDepth()\n","subsample = validated_gbt_model.bestModel.getFeatureSubsetStrategy()\n","\n","print(\"maxDepth of best model -> \", max_depth)\n","print(\"featureSubsetStrategy of best model -> = \", subsample)\n","\n","predictions = validated_gbt_model.transform(X_y_test_vec)    #test on unseen data\n","RMSE = evaluator.evaluate(predictions)    #evaluate predictions using ROOT MEAN SQUARED ERROR\n","print(\"RMSE of best model on unseen data ->  \", RMSE)"],"metadata":{"id":"xSVjnHIfwIz1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_predictions(predictions)    #plot gradient-boosted trees' predicitons"],"metadata":{"id":"ib1Hl4IZwKxx"},"execution_count":null,"outputs":[]}]}