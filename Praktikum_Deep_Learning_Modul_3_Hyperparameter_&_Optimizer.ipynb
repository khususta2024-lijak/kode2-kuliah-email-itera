{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"LuiU3Q426voa","executionInfo":{"status":"ok","timestamp":1727159455321,"user_tz":-420,"elapsed":366,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"outputs":[],"source":["import json\n","import numpy as np\n","import pandas as pd\n","import time\n","\n","# Fungsi aktivasi\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def sigmoid_derivative(x):\n","    return x * (1 - x)\n","\n","# Fungsi update SGD\n","def sgd_update(weight, gradient, learning_rate):\n","    return weight + learning_rate * gradient\n","\n","# Fungsi update Momentum\n","def momentum_update(weight, gradient, learning_rate, velocity, momentum):\n","    velocity = momentum * velocity + learning_rate * gradient\n","    return weight + velocity, velocity\n","\n","# Fungsi update RMSProp\n","def rmsprop_update(weight, gradient, v, learning_rate, beta2, epsilon=1e-8):\n","    v = beta2 * v + (1 - beta2) * (gradient ** 2)\n","    weight -= learning_rate * gradient / (np.sqrt(v) + epsilon)\n","    return weight, v\n","\n","# Fungsi update Adam\n","def adam_update(weight, gradient, m, v, t, learning_rate, beta1, beta2, epsilon=1e-8):\n","    m = beta1 * m + (1 - beta1) * gradient\n","    v = beta2 * v + (1 - beta2) * (gradient ** 2)\n","    m_hat = m / (1 - beta1 ** t)\n","    v_hat = v / (1 - beta2 ** t)\n","    weight += learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n","    return weight, m, v\n","\n","\n","\n","# Neural network class\n","class NeuralNetwork:\n","    def __init__(self, input_size, hidden_size, output_size, optimizer='sgd'):\n","        # Inisialisasi bobot dan bias\n","        self.weights_input_hidden = np.random.randn(input_size, hidden_size)\n","        self.weights_hidden_output = np.random.randn(hidden_size, output_size)\n","        self.bias_hidden = np.random.randn(hidden_size)\n","        self.bias_output = np.random.randn(output_size)\n","\n","        # Setup variabel terkait optimizer\n","        self.optimizer = optimizer\n","        self.velocity_ih = np.zeros_like(self.weights_input_hidden)\n","        self.velocity_ho = np.zeros_like(self.weights_hidden_output)\n","        self.m_ih = np.zeros_like(self.weights_input_hidden)\n","        self.v_ih = np.zeros_like(self.weights_input_hidden)\n","        self.m_ho = np.zeros_like(self.weights_hidden_output)\n","        self.v_ho = np.zeros_like(self.weights_hidden_output)\n","        self.beta1, self.beta2 = 0.9, 0.999\n","        self.t = 1\n","\n","    def forward(self, inputs):\n","        # Propagasi maju untuk batch\n","        self.hidden_input = np.dot(inputs, self.weights_input_hidden) + self.bias_hidden\n","        self.hidden_output = sigmoid(self.hidden_input)\n","        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n","        self.predicted_output = sigmoid(self.output_input)\n","        return self.predicted_output\n","\n","    def backward(self, inputs, targets, learning_rate):\n","        # Backpropagation untuk batch\n","        error = targets - self.predicted_output\n","        delta_output = error * sigmoid_derivative(self.predicted_output)\n","        error_hidden = delta_output.dot(self.weights_hidden_output.T)\n","        delta_hidden = error_hidden * sigmoid_derivative(self.hidden_output)\n","\n","        # Update bobot menggunakan optimizer yang dipilih\n","        if self.optimizer == 'sgd':\n","            self.weights_hidden_output = sgd_update(self.weights_hidden_output,\n","                                                     np.dot(self.hidden_output.T, delta_output), learning_rate)\n","            self.weights_input_hidden = sgd_update(self.weights_input_hidden,\n","                                                    np.dot(inputs.T, delta_hidden), learning_rate)\n","\n","        elif self.optimizer == 'momentum':\n","            self.weights_hidden_output, self.velocity_ho = momentum_update(\n","                self.weights_hidden_output, np.dot(self.hidden_output.T, delta_output),\n","                learning_rate, self.velocity_ho, momentum=0.9\n","            )\n","            self.weights_input_hidden, self.velocity_ih = momentum_update(\n","                self.weights_input_hidden, np.dot(inputs.T, delta_hidden),\n","                learning_rate, self.velocity_ih, momentum=0.9\n","            )\n","\n","        elif self.optimizer == 'adam':\n","            self.weights_hidden_output, self.m_ho, self.v_ho = adam_update(\n","                self.weights_hidden_output, np.dot(self.hidden_output.T, delta_output),\n","                self.m_ho, self.v_ho, self.t, learning_rate, self.beta1, self.beta2\n","            )\n","            self.weights_input_hidden, self.m_ih, self.v_ih = adam_update(\n","                self.weights_input_hidden, np.dot(inputs.T, delta_hidden),\n","                self.m_ih, self.v_ih, self.t, learning_rate, self.beta1, self.beta2\n","            )\n","            self.t += 1\n","\n","        elif self.optimizer == 'rmsprop':\n","            self.weights_hidden_output, self.v_ho = rmsprop_update(\n","                self.weights_hidden_output, np.dot(self.hidden_output.T, delta_output),\n","                self.v_ho, learning_rate, self.beta2\n","            )\n","            self.weights_input_hidden, self.v_ih = rmsprop_update(\n","                self.weights_input_hidden, np.dot(inputs.T, delta_hidden),\n","                self.v_ih, learning_rate, self.beta2\n","            )\n","\n","    def train(self, training_data, targets, epochs, learning_rate, batch_size):\n","        error_history = []  # Untuk menyimpan error di setiap epoch\n","        for epoch in range(epochs):\n","            total_error = 0\n","            for i in range(0, len(training_data), batch_size):\n","                # Buat batch\n","                batch_inputs = training_data[i:i + batch_size]\n","                batch_targets = targets[i:i + batch_size].reshape(-1, 1)\n","\n","                # Forward pass\n","                self.forward(batch_inputs)\n","\n","                # Backward pass\n","                self.backward(batch_inputs, batch_targets, learning_rate)\n","\n","                # Hitung error untuk batch\n","                total_error += np.mean(np.square(batch_targets - self.predicted_output))\n","\n","            error_history.append(total_error / (len(training_data) / batch_size))\n","\n","            # Print error setiap 10 epoch\n","            if epoch % 10 == 0:\n","                print(f\"Epoch {epoch}, Error: {total_error / (len(training_data) / batch_size)}\")\n","        # return error_history\n","\n","    def predict(self, inputs):\n","        return self.forward(inputs)\n","\n"]},{"cell_type":"markdown","source":["### hidden = 8\n","### epoch = 100\n","### batch size = 10\n","### optimizer = adam\n","\n","### disini saya mencoba untuk memperbanyak hidden layer yang saya gunakan, yaitu menjadi 8 hidden layer dengan menggunakan optimizer yang sama yaitu adam. output yang didapatkan menghasilkan error yang terus mengecil pada setiap epoch. tetapi error yang dihasilkan lebih besar dibandingkan dengan menggunakan hidden layer 4"],"metadata":{"id":"A_ZEdR8CyzIl"}},{"cell_type":"code","source":["url = 'https://raw.githubusercontent.com/mirohmi/Heart_Disease_Diagnose/refs/heads/master/heart_diseases.csv'\n","data = pd.read_csv(url)\n","\n","#data = pd.read_csv(\"heart_diseases.csv\")\n","training_data = data.iloc[:, :-1].values\n","targets = data.iloc[:, -1].values\n","\n","# Parameter jaringan saraf\n","input_size = training_data.shape[1]\n","hidden_size = 8  # Jumlah node di hidden layer\n","output_size = 1  # Jumlah output\n","learning_rate = 0.01\n","epochs = 100\n","batch_size = 10\n","\n","nn = NeuralNetwork(input_size, hidden_size, output_size, optimizer='adam')\n","nn.train(training_data, targets, epochs, learning_rate, batch_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2lm34-Lbww1i","executionInfo":{"status":"ok","timestamp":1727161251712,"user_tz":-420,"elapsed":798,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}},"outputId":"dbd412af-c3b0-49f7-fba5-f889d79898e0"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Error: 0.41614975128345855\n","Epoch 10, Error: 0.2390979920297552\n","Epoch 20, Error: 0.24923084538298784\n","Epoch 30, Error: 0.24950467393817838\n","Epoch 40, Error: 0.2494943061376514\n","Epoch 50, Error: 0.24938525058012947\n","Epoch 60, Error: 0.24929893538678946\n","Epoch 70, Error: 0.24923157461791356\n","Epoch 80, Error: 0.24917919671585126\n","Epoch 90, Error: 0.2491383683559075\n"]}]},{"cell_type":"code","source":["url = 'https://raw.githubusercontent.com/mirohmi/Heart_Disease_Diagnose/refs/heads/master/heart_diseases.csv'\n","data = pd.read_csv(url)\n","\n","#data = pd.read_csv(\"heart_diseases.csv\")\n","training_data = data.iloc[:, :-1].values\n","targets = data.iloc[:, -1].values\n","\n","# Parameter jaringan saraf\n","input_size = training_data.shape[1]\n","hidden_size = 8  # Jumlah node di hidden layer\n","output_size = 1  # Jumlah output\n","learning_rate = 0.01\n","epochs = 100\n","batch_size = 1000\n","\n","nn = NeuralNetwork(input_size, hidden_size, output_size, optimizer='adam')\n","nn.train(training_data, targets, epochs, learning_rate, batch_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NJtnQkXo1cy0","executionInfo":{"status":"ok","timestamp":1727161375784,"user_tz":-420,"elapsed":336,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}},"outputId":"ce8f97fe-0501-4082-ae3e-ba9a412027eb"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Error: 1.2175209783878436\n","Epoch 10, Error: 0.9738518464877384\n","Epoch 20, Error: 0.9132411207313392\n","Epoch 30, Error: 0.8735838004994113\n","Epoch 40, Error: 0.8567695734664467\n","Epoch 50, Error: 0.8426215358178044\n","Epoch 60, Error: 0.828232584173308\n","Epoch 70, Error: 0.8110102322468495\n","Epoch 80, Error: 0.7912176458937606\n","Epoch 90, Error: 0.7627870683610734\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-2-a25847be59d5>:8: RuntimeWarning: overflow encountered in exp\n","  return 1 / (1 + np.exp(-x))\n"]}]},{"cell_type":"code","source":["# Test network\n","for inputs in training_data:\n","    prediction = nn.predict(inputs.reshape(1, -1))  # Ubah input ke bentuk batch\n","    print(f\"Predicted Output: {prediction}\")\n","\n","\n","\n","\n","# Fungsi untuk melatih dengan optimizer tertentu\n","def train_with_optimizer(optimizer, input_size, hidden_size, output_size,\n","                         training_data, targets, epochs, learning_rate, batch_size):\n","    # Pilih optimizer langsung di sini\n","    nn = NeuralNetwork(input_size, hidden_size, output_size, optimizer='adam')\n","    start_time = time.time()\n","    error_history = nn.train(training_data, targets, epochs, learning_rate, batch_size)\n","    training_time = time.time() - start_time\n","    return error_history, training_time\n","\n","for inputs in training_data:\n","    prediction = nn.predict(inputs)\n","    print(f\"Input: {inputs}, Predicted Output: {prediction}\")\n","\n","\n","# Latih model dengan berbagai optimizer\n","optimizers = ['sgd', 'adam']\n","results = {}\n","\n","for opt in optimizers:\n","    print(f\"Training with {opt} optimizer...\")\n","    error_history, training_time = train_with_optimizer(\n","        opt, input_size, hidden_size, output_size,\n","        training_data, targets, epochs, learning_rate, batch_size\n","    )\n","    results[opt] = {\n","        'error_history': error_history,\n","        'training_time': training_time\n","    }\n","\n","# Simpan hasil ke file JSON\n","with open('optimizer_results_single_line.json', 'w') as f:\n","    for optimizer, data in results.items():\n","        json_line = json.dumps({optimizer: data}, separators=(',', ':'))\n","        f.write(json_line + '\\n')\n","\n","print(\"Results saved to optimizer_results_single_line.json\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zYKkaUvZvvHo","executionInfo":{"status":"ok","timestamp":1727161044667,"user_tz":-420,"elapsed":1781,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}},"outputId":"158ff7d2-fa35-423a-c4e8-5aa0621abdf1"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Predicted Output: [[0.45082031]]\n","Input: [ 63.    1.    1.  145.  233.    1.    2.  150.    0.    2.3   3.    0.\n","   6. ], Predicted Output: [0.45082031]\n","Input: [ 67.    1.    4.  160.  286.    0.    2.  108.    1.    1.5   2.    3.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 67.    1.    4.  120.  229.    0.    2.  129.    1.    2.6   2.    2.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 37.    1.    3.  130.  250.    0.    0.  187.    0.    3.5   3.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 63.    1.    4.  130.  254.    0.    2.  147.    0.    1.4   2.    1.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 53.    1.    4.  140.  203.    1.    2.  155.    1.    3.1   3.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 57.    1.    4.  140.  192.    0.    0.  148.    0.    0.4   2.    0.\n","   6. ], Predicted Output: [0.45082031]\n","Input: [ 56.    0.    2.  140.  294.    0.    2.  153.    0.    1.3   2.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 56.    1.    3.  130.  256.    1.    2.  142.    1.    0.6   2.    1.\n","   6. ], Predicted Output: [0.45082031]\n","Input: [ 44.   1.   2. 120. 263.   0.   0. 173.   0.   0.   1.   0.   7.], Predicted Output: [0.45082031]\n","Input: [ 52.    1.    3.  172.  199.    1.    0.  162.    0.    0.5   1.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 57.    1.    3.  150.  168.    0.    0.  174.    0.    1.6   1.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 54.    1.    4.  140.  239.    0.    0.  160.    0.    1.2   1.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [4.80e+01 0.00e+00 3.00e+00 1.30e+02 2.75e+02 0.00e+00 0.00e+00 1.39e+02\n"," 0.00e+00 2.00e-01 1.00e+00 0.00e+00 3.00e+00], Predicted Output: [0.45082031]\n","Input: [ 49.    1.    2.  130.  266.    0.    0.  171.    0.    0.6   1.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 60.    1.    4.  130.  206.    0.    2.  132.    1.    2.4   2.    2.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 50.    0.    3.  120.  219.    0.    0.  158.    0.    1.6   2.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 58.   0.   3. 120. 340.   0.   0. 172.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 66.    0.    1.  150.  226.    0.    0.  114.    0.    2.6   3.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 43.    1.    4.  150.  247.    0.    0.  171.    0.    1.5   1.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 40.   1.   4. 110. 167.   0.   2. 114.   1.   2.   2.   0.   7.], Predicted Output: [0.45082031]\n","Input: [ 69.    0.    1.  140.  239.    0.    0.  151.    0.    1.8   1.    2.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 60.    1.    4.  117.  230.    1.    0.  160.    1.    1.4   1.    2.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 64.   1.   3. 140. 335.   0.   0. 158.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 61.   1.   3. 150. 243.   1.   0. 137.   1.   1.   2.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 65.   0.   4. 150. 225.   0.   2. 114.   0.   1.   2.   3.   7.], Predicted Output: [0.45082031]\n","Input: [ 40.    1.    1.  140.  199.    0.    0.  178.    1.    1.4   1.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 71.    0.    2.  160.  302.    0.    0.  162.    0.    0.4   1.    2.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 59.    1.    3.  150.  212.    1.    0.  157.    0.    1.6   1.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 61.   0.   4. 130. 330.   0.   2. 169.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 58.    1.    3.  112.  230.    0.    2.  165.    0.    2.5   2.    1.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 51.    1.    3.  110.  175.    0.    0.  123.    0.    0.6   1.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 50.    1.    4.  150.  243.    0.    2.  128.    0.    2.6   2.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 65.    0.    3.  140.  417.    1.    2.  157.    0.    0.8   1.    1.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 53.    1.    3.  130.  197.    1.    2.  152.    0.    1.2   3.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 41.   0.   2. 105. 198.   0.   0. 168.   0.   0.   1.   1.   3.], Predicted Output: [0.45082031]\n","Input: [ 65.    1.    4.  120.  177.    0.    0.  140.    0.    0.4   1.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 44.   1.   4. 112. 290.   0.   2. 153.   0.   0.   1.   1.   3.], Predicted Output: [0.45082031]\n","Input: [ 44.   1.   2. 130. 219.   0.   2. 188.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 60.    1.    4.  130.  253.    0.    0.  144.    1.    1.4   1.    1.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 54.    1.    4.  124.  266.    0.    2.  109.    1.    2.2   2.    1.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 51.    0.    4.  130.  305.    0.    0.  142.    1.    1.2   2.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 46.    0.    3.  142.  177.    0.    2.  160.    1.    1.4   3.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 58.    1.    4.  128.  216.    0.    2.  131.    1.    2.2   2.    3.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 54.   0.   3. 135. 304.   1.   0. 170.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 54.    1.    4.  120.  188.    0.    0.  113.    0.    1.4   2.    1.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 60.    1.    4.  145.  282.    0.    2.  142.    1.    2.8   2.    2.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 60.   1.   3. 140. 185.   0.   2. 155.   0.   3.   2.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 54.    1.    3.  150.  232.    0.    2.  165.    0.    1.6   1.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 59.    1.    4.  170.  326.    0.    2.  140.    1.    3.4   3.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 46.    1.    3.  150.  231.    0.    0.  147.    0.    3.6   2.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 65.    0.    3.  155.  269.    0.    0.  148.    0.    0.8   1.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [6.70e+01 1.00e+00 4.00e+00 1.25e+02 2.54e+02 1.00e+00 0.00e+00 1.63e+02\n"," 0.00e+00 2.00e-01 2.00e+00 2.00e+00 7.00e+00], Predicted Output: [0.45082031]\n","Input: [ 62.    1.    4.  120.  267.    0.    0.   99.    1.    1.8   2.    2.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 65.    1.    4.  110.  248.    0.    2.  158.    0.    0.6   1.    2.\n","   6. ], Predicted Output: [0.45082031]"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-2-a25847be59d5>:8: RuntimeWarning: overflow encountered in exp\n","  return 1 / (1 + np.exp(-x))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Input: [ 44.   1.   4. 110. 197.   0.   2. 177.   0.   0.   1.   1.   3.], Predicted Output: [0.45082031]\n","Input: [ 65.    0.    3.  160.  360.    0.    2.  151.    0.    0.8   1.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 60.    1.    4.  125.  258.    0.    2.  141.    1.    2.8   2.    1.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 51.    0.    3.  140.  308.    0.    2.  142.    0.    1.5   1.    1.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [4.80e+01 1.00e+00 2.00e+00 1.30e+02 2.45e+02 0.00e+00 2.00e+00 1.80e+02\n"," 0.00e+00 2.00e-01 2.00e+00 0.00e+00 3.00e+00], Predicted Output: [0.45082031]\n","Input: [ 68.    1.    3.  180.  274.    1.    2.  150.    1.    1.6   2.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [5.20e+01 1.00e+00 2.00e+00 1.20e+02 3.25e+02 0.00e+00 0.00e+00 1.72e+02\n"," 0.00e+00 2.00e-01 1.00e+00 0.00e+00 3.00e+00], Predicted Output: [0.45082031]\n","Input: [ 44.   1.   3. 140. 235.   0.   2. 180.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 47.   1.   3. 138. 257.   0.   2. 156.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 53.   0.   4. 138. 234.   0.   2. 160.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 51.    0.    3.  130.  256.    0.    2.  149.    0.    0.5   1.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 66.    1.    4.  120.  302.    0.    2.  151.    0.    0.4   2.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 62.    0.    4.  160.  164.    0.    2.  145.    0.    6.2   3.    3.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 62.    1.    3.  130.  231.    0.    0.  146.    0.    1.8   2.    3.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 44.    0.    3.  108.  141.    0.    0.  175.    0.    0.6   2.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 59.    1.    4.  110.  239.    0.    2.  142.    1.    1.2   2.    1.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 60.    0.    4.  150.  258.    0.    2.  157.    0.    2.6   2.    2.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 52.    1.    2.  134.  201.    0.    0.  158.    0.    0.8   1.    1.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 48.   1.   4. 122. 222.   0.   2. 186.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 45.   1.   4. 115. 260.   0.   2. 185.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 34.   1.   1. 118. 182.   0.   2. 174.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 57.   0.   4. 128. 303.   0.   2. 159.   0.   0.   1.   1.   3.], Predicted Output: [0.45082031]\n","Input: [ 71.   0.   3. 110. 265.   1.   2. 130.   0.   0.   1.   1.   3.], Predicted Output: [0.45082031]\n","Input: [ 49.   1.   3. 120. 188.   0.   0. 139.   0.   2.   2.   3.   7.], Predicted Output: [0.45082031]\n","Input: [ 54.   1.   2. 108. 309.   0.   0. 156.   0.   0.   1.   0.   7.], Predicted Output: [0.45082031]\n","Input: [ 59.   1.   4. 140. 177.   0.   0. 162.   1.   0.   1.   1.   7.], Predicted Output: [0.45082031]\n","Input: [ 57.    1.    3.  128.  229.    0.    2.  150.    0.    0.4   2.    1.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 61.    1.    4.  120.  260.    0.    0.  140.    1.    3.6   2.    1.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 39.    1.    4.  118.  219.    0.    0.  140.    0.    1.2   2.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 61.   0.   4. 145. 307.   0.   2. 146.   1.   1.   2.   0.   7.], Predicted Output: [0.45082031]\n","Input: [ 56.    1.    4.  125.  249.    1.    2.  144.    1.    1.2   2.    1.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 62.    0.    3.  130.  263.    0.    0.   97.    0.    1.2   2.    1.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 41.   1.   2. 135. 203.   0.   0. 132.   0.   0.   2.   0.   6.], Predicted Output: [0.45082031]\n","Input: [ 58.   1.   3. 140. 211.   1.   2. 165.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 35.    0.    4.  138.  183.    0.    0.  182.    0.    1.4   1.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 63.    1.    4.  130.  330.    1.    2.  132.    1.    1.8   1.    3.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 51.    1.    3.  100.  222.    0.    0.  143.    1.    1.2   2.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 55.    1.    4.  140.  217.    0.    0.  111.    1.    5.6   3.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 65.    1.    1.  138.  282.    1.    2.  174.    0.    1.4   2.    1.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 45.    0.    2.  130.  234.    0.    2.  175.    0.    0.6   2.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 56.   0.   4. 200. 288.   1.   2. 133.   1.   4.   3.   2.   7.], Predicted Output: [0.45082031]\n","Input: [ 54.    1.    4.  110.  239.    0.    0.  126.    1.    2.8   2.    1.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 44.   1.   2. 120. 220.   0.   0. 170.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 62.   0.   4. 124. 209.   0.   0. 163.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 54.    1.    3.  120.  258.    0.    2.  147.    0.    0.4   2.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 51.   1.   3.  94. 227.   0.   0. 154.   1.   0.   1.   1.   7.], Predicted Output: [0.45082031]\n","Input: [ 29.   1.   2. 130. 204.   0.   2. 202.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 51.   1.   4. 140. 261.   0.   2. 186.   1.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [4.30e+01 0.00e+00 3.00e+00 1.22e+02 2.13e+02 0.00e+00 0.00e+00 1.65e+02\n"," 0.00e+00 2.00e-01 2.00e+00 0.00e+00 3.00e+00], Predicted Output: [0.45082031]\n","Input: [ 55.    0.    2.  135.  250.    0.    2.  161.    0.    1.4   2.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 70.    1.    4.  145.  174.    0.    0.  125.    1.    2.6   3.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 62.    1.    2.  120.  281.    0.    2.  103.    0.    1.4   2.    1.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 35.    1.    4.  120.  198.    0.    0.  130.    1.    1.6   2.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 51.    1.    3.  125.  245.    1.    2.  166.    0.    2.4   2.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 59.   1.   2. 140. 221.   0.   0. 164.   1.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [5.90e+01 1.00e+00 1.00e+00 1.70e+02 2.88e+02 0.00e+00 2.00e+00 1.59e+02\n"," 0.00e+00 2.00e-01 2.00e+00 0.00e+00 7.00e+00], Predicted Output: [0.45082031]\n","Input: [ 52.   1.   2. 128. 205.   1.   0. 184.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 64.    1.    3.  125.  309.    0.    0.  131.    1.    1.8   2.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 58.    1.    3.  105.  240.    0.    2.  154.    1.    0.6   2.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 47.   1.   3. 108. 243.   0.   0. 152.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 57.   1.   4. 165. 289.   1.   2. 124.   0.   1.   2.   3.   7.], Predicted Output: [0.45082031]\n","Input: [ 41.   1.   3. 112. 250.   0.   0. 179.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 45.   1.   2. 128. 308.   0.   2. 170.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 60.   0.   3. 102. 318.   0.   0. 160.   0.   0.   1.   1.   3.], Predicted Output: [0.45082031]\n","Input: [ 52.    1.    1.  152.  298.    1.    0.  178.    0.    1.2   2.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 42.    0.    4.  102.  265.    0.    2.  122.    0.    0.6   2.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 67.    0.    3.  115.  564.    0.    2.  160.    0.    1.6   2.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 55.    1.    4.  160.  289.    0.    2.  145.    1.    0.8   2.    1.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 64.    1.    4.  120.  246.    0.    2.   96.    1.    2.2   3.    1.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 70.    1.    4.  130.  322.    0.    2.  109.    0.    2.4   2.    3.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 51.    1.    4.  140.  299.    0.    0.  173.    1.    1.6   1.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 58.   1.   4. 125. 300.   0.   2. 171.   0.   0.   1.   2.   7.], Predicted Output: [0.45082031]\n","Input: [ 60.    1.    4.  140.  293.    0.    2.  170.    0.    1.2   2.    2.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 68.   1.   3. 118. 277.   0.   0. 151.   0.   1.   1.   1.   7.], Predicted Output: [0.45082031]\n","Input: [ 46.   1.   2. 101. 197.   1.   0. 156.   0.   0.   1.   0.   7.], Predicted Output: [0.45082031]\n","Input: [ 77.   1.   4. 125. 304.   0.   2. 162.   1.   0.   1.   3.   3.], Predicted Output: [0.45082031]\n","Input: [ 54.    0.    3.  110.  214.    0.    0.  158.    0.    1.6   2.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 58.   0.   4. 100. 248.   0.   2. 122.   0.   1.   2.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 48.   1.   3. 124. 255.   1.   0. 175.   0.   0.   1.   2.   3.], Predicted Output: [0.45082031]\n","Input: [ 57.   1.   4. 132. 207.   0.   0. 168.   1.   0.   1.   0.   7.], Predicted Output: [0.45082031]\n","Input: [ 54.   0.   2. 132. 288.   1.   2. 159.   1.   0.   1.   1.   3.], Predicted Output: [0.45082031]\n","Input: [ 35.   1.   4. 126. 282.   0.   2. 156.   1.   0.   1.   0.   7.], Predicted Output: [0.45082031]\n","Input: [ 45.   0.   2. 112. 160.   0.   0. 138.   0.   0.   2.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 70.    1.    3.  160.  269.    0.    0.  112.    1.    2.9   2.    1.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 53.   1.   4. 142. 226.   0.   2. 111.   1.   0.   1.   0.   7.], Predicted Output: [0.45082031]\n","Input: [ 59.   0.   4. 174. 249.   0.   0. 143.   1.   0.   2.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 62.    0.    4.  140.  394.    0.    2.  157.    0.    1.2   2.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 64.   1.   4. 145. 212.   0.   2. 132.   0.   2.   2.   2.   6.], Predicted Output: [0.45082031]\n","Input: [ 57.    1.    4.  152.  274.    0.    0.   88.    1.    1.2   2.    1.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [5.20e+01 1.00e+00 4.00e+00 1.08e+02 2.33e+02 1.00e+00 0.00e+00 1.47e+02\n"," 0.00e+00 1.00e-01 1.00e+00 3.00e+00 7.00e+00], Predicted Output: [0.45082031]\n","Input: [ 56.    1.    4.  132.  184.    0.    2.  105.    1.    2.1   2.    1.\n","   6. ], Predicted Output: [0.45082031]\n","Input: [ 43.    1.    3.  130.  315.    0.    0.  162.    0.    1.9   1.    1.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 53.   1.   3. 130. 246.   1.   2. 173.   0.   0.   1.   3.   3.], Predicted Output: [0.45082031]\n","Input: [ 48.    1.    4.  124.  274.    0.    2.  166.    0.    0.5   2.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 56.    0.    4.  134.  409.    0.    2.  150.    1.    1.9   2.    2.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 42.    1.    1.  148.  244.    0.    2.  178.    0.    0.8   1.    2.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 59.    1.    1.  178.  270.    0.    2.  145.    0.    4.2   3.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 60.   0.   4. 158. 305.   0.   2. 161.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 63.   0.   2. 140. 195.   0.   0. 179.   0.   0.   1.   2.   3.], Predicted Output: [0.45082031]\n","Input: [ 42.    1.    3.  120.  240.    1.    0.  194.    0.    0.8   3.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 66.   1.   2. 160. 246.   0.   0. 120.   1.   0.   2.   3.   6.], Predicted Output: [0.45082031]\n","Input: [ 54.   1.   2. 192. 283.   0.   2. 195.   0.   0.   1.   1.   7.], Predicted Output: [0.45082031]\n","Input: [ 69.   1.   3. 140. 254.   0.   2. 146.   0.   2.   2.   3.   7.], Predicted Output: [0.45082031]\n","Input: [ 50.   1.   3. 129. 196.   0.   0. 163.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 51.    1.    4.  140.  298.    0.    0.  122.    1.    4.2   2.    3.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 62.    0.    4.  138.  294.    1.    0.  106.    0.    1.9   2.    3.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 68.    0.    3.  120.  211.    0.    2.  115.    0.    1.5   2.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 67.    1.    4.  100.  299.    0.    2.  125.    1.    0.9   2.    2.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [6.90e+01 1.00e+00 1.00e+00 1.60e+02 2.34e+02 1.00e+00 2.00e+00 1.31e+02\n"," 0.00e+00 1.00e-01 2.00e+00 1.00e+00 3.00e+00], Predicted Output: [0.45082031]\n","Input: [4.50e+01 0.00e+00 4.00e+00 1.38e+02 2.36e+02 0.00e+00 2.00e+00 1.52e+02\n"," 1.00e+00 2.00e-01 2.00e+00 0.00e+00 3.00e+00], Predicted Output: [0.45082031]\n","Input: [ 50.    0.    2.  120.  244.    0.    0.  162.    0.    1.1   1.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 59.   1.   1. 160. 273.   0.   2. 125.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 50.   0.   4. 110. 254.   0.   2. 159.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 64.   0.   4. 180. 325.   0.   0. 154.   1.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 57.    1.    3.  150.  126.    1.    0.  173.    0.    0.2   1.    1.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [6.40e+01 0.00e+00 3.00e+00 1.40e+02 3.13e+02 0.00e+00 0.00e+00 1.33e+02\n"," 0.00e+00 2.00e-01 1.00e+00 0.00e+00 7.00e+00], Predicted Output: [0.45082031]\n","Input: [ 43.   1.   4. 110. 211.   0.   0. 161.   0.   0.   1.   0.   7.], Predicted Output: [0.45082031]\n","Input: [ 45.   1.   4. 142. 309.   0.   2. 147.   1.   0.   2.   3.   7.], Predicted Output: [0.45082031]\n","Input: [ 58.   1.   4. 128. 259.   0.   2. 130.   1.   3.   2.   2.   7.], Predicted Output: [0.45082031]\n","Input: [ 50.    1.    4.  144.  200.    0.    2.  126.    1.    0.9   2.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 55.   1.   2. 130. 262.   0.   0. 155.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 62.    0.    4.  150.  244.    0.    0.  154.    1.    1.4   2.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 37.   0.   3. 120. 215.   0.   0. 170.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 38.    1.    1.  120.  231.    0.    0.  182.    1.    3.8   2.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 41.   1.   3. 130. 214.   0.   2. 168.   0.   2.   2.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 66.   0.   4. 178. 228.   1.   0. 165.   1.   1.   2.   2.   7.], Predicted Output: [0.45082031]\n","Input: [ 52.   1.   4. 112. 230.   0.   0. 160.   0.   0.   1.   1.   3.], Predicted Output: [0.45082031]\n","Input: [ 56.    1.    1.  120.  193.    0.    2.  162.    0.    1.9   2.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 46.   0.   2. 105. 204.   0.   0. 172.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 46.   0.   4. 138. 243.   0.   2. 152.   1.   0.   2.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 64.   0.   4. 130. 303.   0.   0. 122.   0.   2.   2.   2.   3.], Predicted Output: [0.45082031]\n","Input: [ 59.   1.   4. 138. 271.   0.   2. 182.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 41.   0.   3. 112. 268.   0.   2. 172.   1.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 54.   0.   3. 108. 267.   0.   2. 167.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 39.   0.   3.  94. 199.   0.   0. 179.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 53.   1.   4. 123. 282.   0.   0.  95.   1.   2.   2.   2.   7.], Predicted Output: [0.45082031]\n","Input: [ 63.    0.    4.  108.  269.    0.    0.  169.    1.    1.8   2.    2.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 34.    0.    2.  118.  210.    0.    0.  192.    0.    0.7   1.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [4.70e+01 1.00e+00 4.00e+00 1.12e+02 2.04e+02 0.00e+00 0.00e+00 1.43e+02\n"," 0.00e+00 1.00e-01 1.00e+00 0.00e+00 3.00e+00], Predicted Output: [0.45082031]\n","Input: [ 67.   0.   3. 152. 277.   0.   0. 172.   0.   0.   1.   1.   3.], Predicted Output: [0.45082031]\n","Input: [ 54.   1.   4. 110. 206.   0.   2. 108.   1.   0.   2.   1.   3.], Predicted Output: [0.45082031]\n","Input: [6.60e+01 1.00e+00 4.00e+00 1.12e+02 2.12e+02 0.00e+00 2.00e+00 1.32e+02\n"," 1.00e+00 1.00e-01 1.00e+00 1.00e+00 3.00e+00], Predicted Output: [0.45082031]\n","Input: [5.20e+01 0.00e+00 3.00e+00 1.36e+02 1.96e+02 0.00e+00 2.00e+00 1.69e+02\n"," 0.00e+00 1.00e-01 2.00e+00 0.00e+00 3.00e+00], Predicted Output: [0.45082031]\n","Input: [ 55.    0.    4.  180.  327.    0.    1.  117.    1.    3.4   2.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 49.    1.    3.  118.  149.    0.    2.  126.    0.    0.8   1.    3.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [7.40e+01 0.00e+00 2.00e+00 1.20e+02 2.69e+02 0.00e+00 2.00e+00 1.21e+02\n"," 1.00e+00 2.00e-01 1.00e+00 1.00e+00 3.00e+00], Predicted Output: [0.45082031]\n","Input: [ 54.   0.   3. 160. 201.   0.   0. 163.   0.   0.   1.   1.   3.], Predicted Output: [0.45082031]\n","Input: [ 54.    1.    4.  122.  286.    0.    2.  116.    1.    3.2   2.    2.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 56.    1.    4.  130.  283.    1.    2.  103.    1.    1.6   3.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 46.    1.    4.  120.  249.    0.    2.  144.    0.    0.8   1.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 49.   0.   2. 134. 271.   0.   0. 162.   0.   0.   2.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 42.   1.   2. 120. 295.   0.   0. 162.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 41.   1.   2. 110. 235.   0.   0. 153.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 41.   0.   2. 126. 306.   0.   0. 163.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 49.   0.   4. 130. 269.   0.   0. 163.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 61.    1.    1.  134.  234.    0.    0.  145.    0.    2.6   2.    2.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 60.   0.   3. 120. 178.   1.   0.  96.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 67.   1.   4. 120. 237.   0.   0.  71.   0.   1.   2.   0.   3.], Predicted Output: [0.45082031]\n","Input: [5.80e+01 1.00e+00 4.00e+00 1.00e+02 2.34e+02 0.00e+00 0.00e+00 1.56e+02\n"," 0.00e+00 1.00e-01 1.00e+00 1.00e+00 7.00e+00], Predicted Output: [0.45082031]\n","Input: [ 47.   1.   4. 110. 275.   0.   2. 118.   1.   1.   2.   1.   3.], Predicted Output: [0.45082031]\n","Input: [ 52.   1.   4. 125. 212.   0.   0. 168.   0.   1.   1.   2.   7.], Predicted Output: [0.45082031]\n","Input: [ 62.   1.   2. 128. 208.   1.   2. 140.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 57.    1.    4.  110.  201.    0.    0.  126.    1.    1.5   2.    0.\n","   6. ], Predicted Output: [0.45082031]\n","Input: [ 58.   1.   4. 146. 218.   0.   0. 105.   0.   2.   2.   1.   7.], Predicted Output: [0.45082031]\n","Input: [6.40e+01 1.00e+00 4.00e+00 1.28e+02 2.63e+02 0.00e+00 0.00e+00 1.05e+02\n"," 1.00e+00 2.00e-01 2.00e+00 1.00e+00 7.00e+00], Predicted Output: [0.45082031]\n","Input: [ 51.    0.    3.  120.  295.    0.    2.  157.    0.    0.6   1.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 43.    1.    4.  115.  303.    0.    0.  181.    0.    1.2   2.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 42.   0.   3. 120. 209.   0.   0. 173.   0.   0.   2.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 67.    0.    4.  106.  223.    0.    0.  142.    0.    0.3   1.    2.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 76.    0.    3.  140.  197.    0.    1.  116.    0.    1.1   2.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 70.   1.   2. 156. 245.   0.   2. 143.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 57.    1.    2.  124.  261.    0.    0.  141.    0.    0.3   1.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 44.    0.    3.  118.  242.    0.    0.  149.    0.    0.3   2.    1.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 58.   0.   2. 136. 319.   1.   2. 152.   0.   0.   1.   2.   3.], Predicted Output: [0.45082031]\n","Input: [ 60.    0.    1.  150.  240.    0.    0.  171.    0.    0.9   1.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 44.   1.   3. 120. 226.   0.   0. 169.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 61.    1.    4.  138.  166.    0.    2.  125.    1.    3.6   2.    1.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 42.    1.    4.  136.  315.    0.    0.  125.    1.    1.8   2.    0.\n","   6. ], Predicted Output: [0.45082031]\n","Input: [ 59.    1.    3.  126.  218.    1.    0.  134.    0.    2.2   2.    1.\n","   6. ], Predicted Output: [0.45082031]\n","Input: [ 40.   1.   4. 152. 223.   0.   0. 181.   0.   0.   1.   0.   7.], Predicted Output: [0.45082031]\n","Input: [ 42.   1.   3. 130. 180.   0.   0. 150.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 61.    1.    4.  140.  207.    0.    2.  138.    1.    1.9   1.    1.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 66.    1.    4.  160.  228.    0.    2.  138.    0.    2.3   1.    0.\n","   6. ], Predicted Output: [0.45082031]\n","Input: [ 46.    1.    4.  140.  311.    0.    0.  120.    1.    1.8   2.    2.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 71.    0.    4.  112.  149.    0.    0.  125.    0.    1.6   2.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 59.    1.    1.  134.  204.    0.    0.  162.    0.    0.8   1.    2.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 64.    1.    1.  170.  227.    0.    2.  155.    0.    0.6   2.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 66.   0.   3. 146. 278.   0.   2. 152.   0.   0.   2.   1.   3.], Predicted Output: [0.45082031]\n","Input: [ 39.   0.   3. 138. 220.   0.   0. 152.   0.   0.   2.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 57.   1.   2. 154. 232.   0.   2. 164.   0.   0.   1.   1.   3.], Predicted Output: [0.45082031]\n","Input: [ 58.    0.    4.  130.  197.    0.    0.  131.    0.    0.6   2.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 57.   1.   4. 110. 335.   0.   0. 143.   1.   3.   2.   1.   7.], Predicted Output: [0.45082031]\n","Input: [ 47.   1.   3. 130. 253.   0.   0. 179.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 55.   0.   4. 128. 205.   0.   1. 130.   1.   2.   2.   1.   7.], Predicted Output: [0.45082031]\n","Input: [ 35.   1.   2. 122. 192.   0.   0. 174.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 61.   1.   4. 148. 203.   0.   0. 161.   0.   0.   1.   1.   7.], Predicted Output: [0.45082031]\n","Input: [ 58.    1.    4.  114.  318.    0.    1.  140.    0.    4.4   3.    3.\n","   6. ], Predicted Output: [0.45082031]\n","Input: [ 58.    0.    4.  170.  225.    1.    2.  146.    1.    2.8   2.    2.\n","   6. ], Predicted Output: [0.45082031]\n","Input: [ 56.   1.   2. 130. 221.   0.   2. 163.   0.   0.   1.   0.   7.], Predicted Output: [0.45082031]\n","Input: [ 56.   1.   2. 120. 240.   0.   0. 169.   0.   0.   3.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 67.    1.    3.  152.  212.    0.    2.  150.    0.    0.8   2.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 55.    0.    2.  132.  342.    0.    0.  166.    0.    1.2   1.    0.\n","   3. ], Predicted Output: [0.45082031]\n","Input: [ 44.    1.    4.  120.  169.    0.    0.  144.    1.    2.8   3.    0.\n","   6. ], Predicted Output: [0.45082031]\n","Input: [ 63.   1.   4. 140. 187.   0.   2. 144.   1.   4.   1.   2.   7.], Predicted Output: [0.45082031]\n","Input: [ 63.   0.   4. 124. 197.   0.   0. 136.   1.   0.   2.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 41.   1.   2. 120. 157.   0.   0. 182.   0.   0.   1.   0.   3.], Predicted Output: [0.45082031]\n","Input: [ 59.   1.   4. 164. 176.   1.   2.  90.   0.   1.   2.   2.   6.], Predicted Output: [0.45082031]\n","Input: [5.70e+01 0.00e+00 4.00e+00 1.40e+02 2.41e+02 0.00e+00 0.00e+00 1.23e+02\n"," 1.00e+00 2.00e-01 2.00e+00 0.00e+00 7.00e+00], Predicted Output: [0.45082031]\n","Input: [ 45.    1.    1.  110.  264.    0.    0.  132.    0.    1.2   2.    0.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 68.    1.    4.  144.  193.    1.    0.  141.    0.    3.4   2.    2.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 57.    1.    4.  130.  131.    0.    0.  115.    1.    1.2   2.    1.\n","   7. ], Predicted Output: [0.45082031]\n","Input: [ 57.   0.   2. 130. 236.   0.   2. 174.   0.   0.   2.   1.   3.], Predicted Output: [0.45082031]\n","Training with sgd optimizer...\n","Epoch 0, Error: 0.25085693797801767\n","Epoch 10, Error: 0.24542027540387348\n","Epoch 20, Error: 0.23805201061821743\n","Epoch 30, Error: 0.22720159064140621\n","Epoch 40, Error: 0.22514866474114636\n","Epoch 50, Error: 0.23417485155004691\n","Epoch 60, Error: 0.23585628958200386\n","Epoch 70, Error: 0.22458537248186555\n","Epoch 80, Error: 0.20885258071673654\n","Epoch 90, Error: 0.1971654367946233\n","Training with adam optimizer...\n","Epoch 0, Error: 0.2582041402578162\n","Epoch 10, Error: 0.20921323521873111\n","Epoch 20, Error: 0.1966413910468497\n","Epoch 30, Error: 0.18584818181546725\n","Epoch 40, Error: 0.1935676995201314\n","Epoch 50, Error: 0.19636015089158315\n","Epoch 60, Error: 0.18540272731112345\n","Epoch 70, Error: 0.17933282462058772\n","Epoch 80, Error: 0.1797254553877765\n","Epoch 90, Error: 0.17598266251694508\n","Results saved to optimizer_results_single_line.json\n"]}]},{"cell_type":"markdown","source":["## Latihan Mandiri"],"metadata":{"id":"jqQgRsfFs9P9"}},{"cell_type":"markdown","source":["### hidden = 4\n","### epoch = 100\n","### batch size = 10\n","### optimizer = adam\n","\n","### output yang dihasilkan adalah pada setiap epoch error yang didapatkan semakin kecil"],"metadata":{"id":"05V7iN0Pxc91"}},{"cell_type":"code","source":["url = 'https://raw.githubusercontent.com/mirohmi/Heart_Disease_Diagnose/refs/heads/master/heart_diseases.csv'\n","data = pd.read_csv(url)\n","\n","#data = pd.read_csv(\"heart_diseases.csv\")\n","training_data = data.iloc[:, :-1].values\n","targets = data.iloc[:, -1].values\n","\n","# Parameter jaringan saraf\n","input_size = training_data.shape[1]\n","hidden_size = 4  # Jumlah node di hidden layer\n","output_size = 1  # Jumlah output\n","learning_rate = 0.01\n","epochs = 100\n","batch_size = 10\n","\n","nn = NeuralNetwork(input_size, hidden_size, output_size, optimizer='adam')\n","nn.train(training_data, targets, epochs, learning_rate, batch_size)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hyKpEmXEun40","executionInfo":{"status":"ok","timestamp":1727160109538,"user_tz":-420,"elapsed":727,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}},"outputId":"83850450-cdb0-49d9-bde6-009f6bd3b8e8"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-2-a25847be59d5>:8: RuntimeWarning: overflow encountered in exp\n","  return 1 / (1 + np.exp(-x))\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0, Error: 0.3149901556293327\n","Epoch 10, Error: 0.2531418228980655\n","Epoch 20, Error: 0.25231886587921776\n","Epoch 30, Error: 0.2518884921930473\n","Epoch 40, Error: 0.25166051801907685\n","Epoch 50, Error: 0.25155466975088375\n","Epoch 60, Error: 0.25150733884032694\n","Epoch 70, Error: 0.25148602611117965\n","Epoch 80, Error: 0.25147632758968547\n","Epoch 90, Error: 0.2514719562638033\n"]}]},{"cell_type":"markdown","source":["### hidden = 4\n","### epoch = 100\n","### batch size = 10\n","### optimizer = sgd\n","\n","### disini saya mencoba untuk menggunakan oprimizer yang berbeda yaitu sgd. dari hasil perbandingan dengan menggunaan adam dan sgd didapatkanlah perbandingan bahwa output yang dihasilkan oleh sgd lebih kecil dibandingkan dengan menggunakan adam."],"metadata":{"id":"FWQApqVczo5I"}},{"cell_type":"code","source":["url = 'https://raw.githubusercontent.com/mirohmi/Heart_Disease_Diagnose/refs/heads/master/heart_diseases.csv'\n","data = pd.read_csv(url)\n","\n","#data = pd.read_csv(\"heart_diseases.csv\")\n","training_data = data.iloc[:, :-1].values\n","targets = data.iloc[:, -1].values\n","\n","# Parameter jaringan saraf\n","input_size = training_data.shape[1]\n","hidden_size = 4  # Jumlah node di hidden layer\n","output_size = 1  # Jumlah output\n","learning_rate = 0.01\n","epochs = 100\n","batch_size = 10\n","\n","nn = NeuralNetwork(input_size, hidden_size, output_size, optimizer='sgd')\n","nn.train(training_data, targets, epochs, learning_rate, batch_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IfNCtG9nxRjj","executionInfo":{"status":"ok","timestamp":1727160212868,"user_tz":-420,"elapsed":372,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}},"outputId":"d4587908-1cbf-4caa-d280-4da802bc5962"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-2-a25847be59d5>:8: RuntimeWarning: overflow encountered in exp\n","  return 1 / (1 + np.exp(-x))\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0, Error: 0.26537451532890516\n","Epoch 10, Error: 0.25218847680279577\n","Epoch 20, Error: 0.25147399363492057\n","Epoch 30, Error: 0.25142019073548116\n","Epoch 40, Error: 0.25140321250893416\n","Epoch 50, Error: 0.2513887822430149\n","Epoch 60, Error: 0.2513736831103566\n","Epoch 70, Error: 0.2513576918681557\n","Epoch 80, Error: 0.2513412063955576\n","Epoch 90, Error: 0.25132469679239644\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"oGVuDf6qtZCP"}},{"cell_type":"code","source":[],"metadata":{"id":"f1leZCOotW_j"},"execution_count":null,"outputs":[]}]}