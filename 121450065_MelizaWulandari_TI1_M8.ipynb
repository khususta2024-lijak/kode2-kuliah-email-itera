{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nfcxg8kuIxt"
      },
      "source": [
        "# Modul 8 Analisis Big Data\n",
        "# Time Series Analysis dengan PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36WijAJluIxx"
      },
      "source": [
        "#Import useful libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBptDXMvvbQ7",
        "outputId": "24f31c4c-24e9-4866-ca72-71a861bcc862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=6b988507945d55fce97abe06fc352e1f3ef43fe6aab4b90d89e7174dbb868cef\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "menginstal pyspark terlebih dahulu menggunakan pip install pyspark"
      ],
      "metadata": {
        "id": "UOWobU7qPLI3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHekvw19uIx0"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "kemudian import library yang diperlukan"
      ],
      "metadata": {
        "id": "NRUCCoDXPnMj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOBcWMDzuIx4"
      },
      "source": [
        "#Read dataset from csv\n",
        "\n",
        "**NB**: For some reason yahoo finance responded with 403 when trying to download the dataset directly with wget, so the file \"BTC_USD.csv\" (included in the archive) needs to be added to DBFS at \"dbfs:/FileStore/BTC_USD/BTC_USD.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB4UJ3xJuIx4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f83dbc83-d60c-4cf2-d28e-f37c11480b9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: date (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Adj Close: double (nullable = true)\n",
            " |-- Volume: long (nullable = true)\n",
            "\n",
            "+----------+----------+----------+----------+----------+----------+--------+\n",
            "|      Date|      Open|      High|       Low|     Close| Adj Close|  Volume|\n",
            "+----------+----------+----------+----------+----------+----------+--------+\n",
            "|2014-09-17|465.864014|468.174011|452.421997|457.334015|457.334015|21056800|\n",
            "|2014-09-18|456.859985|456.859985|413.104004|424.440002|424.440002|34483200|\n",
            "|2014-09-19|424.102997|427.834991|384.532013| 394.79599| 394.79599|37919700|\n",
            "|2014-09-20|394.673004| 423.29599|389.882996|408.903992|408.903992|36863600|\n",
            "|2014-09-21|408.084991|412.425995|   393.181|398.821014|398.821014|26580100|\n",
            "|2014-09-22|399.100006|406.915985|397.130005|402.152008|402.152008|24127600|\n",
            "|2014-09-23| 402.09201|441.557007|396.196991|435.790985|435.790985|45099500|\n",
            "|2014-09-24|435.751007|   436.112|421.131989|423.204987|423.204987|30627700|\n",
            "|2014-09-25|423.156006|423.519989|409.467987|411.574005|411.574005|26814400|\n",
            "|2014-09-26|411.428986|414.937988|400.009003|404.424988|404.424988|21460800|\n",
            "|2014-09-27|   403.556|406.622986|397.372009|399.519989|399.519989|15029300|\n",
            "|2014-09-28|399.471008|401.016998|374.332001|   377.181|   377.181|23613300|\n",
            "|2014-09-29|376.928009|385.210999| 372.23999| 375.46701| 375.46701|32497700|\n",
            "|2014-09-30|376.088013| 390.97699|373.442993|   386.944|   386.944|34707300|\n",
            "|2014-10-01|387.427002|391.378998|380.779999| 383.61499| 383.61499|26229400|\n",
            "|2014-10-02|383.988007|385.497009|372.946014|375.071991|375.071991|21777700|\n",
            "|2014-10-03|   375.181|377.695007|357.859009|359.511993|359.511993|30901200|\n",
            "|2014-10-04|359.891998|   364.487|325.885986|328.865997|328.865997|47236500|\n",
            "|2014-10-05|328.915985|341.800995| 289.29599| 320.51001| 320.51001|83308096|\n",
            "|2014-10-06|320.389008|345.134003|302.559998| 330.07901| 330.07901|79011800|\n",
            "+----------+----------+----------+----------+----------+----------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"YourAppName\") \\\n",
        "    .getOrCreate()\n",
        "#read from dbfs\n",
        "df = spark.read.load(\"/content/BTC-USD.csv\",\n",
        "                      format=\"csv\",\n",
        "                      sep=\",\",\n",
        "                      inferSchema=\"true\",\n",
        "                      header=\"true\");\n",
        "\n",
        "# Show the DataFrame schema\n",
        "df.printSchema()\n",
        "\n",
        "# Show the first few rows of the DataFrame\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lalu import data dan tampilkan data dalam bentuk dataframe schema"
      ],
      "metadata": {
        "id": "K-TBQRY0Pwy6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZKth7ozuIx5"
      },
      "source": [
        "#Train-test split\n",
        "\n",
        "Validation not needed because CrossValidator will use part of train set as validation set (KFold)\n",
        "\n",
        "Proportion of split is ~ 70/30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUD7fzQeuIx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71c7d267-6ca9-47a4-c4c5-31ea579c8181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in train set: 1826\n",
            "Number of rows in test set: 861\n"
          ]
        }
      ],
      "source": [
        "train_set = df.filter(col(\"Date\") < \"2019-09-17\")\n",
        "test_set = df.filter(col(\"Date\") >= \"2019-09-17\")\n",
        "\n",
        "# Show the number of rows in each set\n",
        "print(\"Number of rows in train set:\", train_set.count())\n",
        "print(\"Number of rows in test set:\", test_set.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lalu bagi dataset menjadi data uji dan data latih"
      ],
      "metadata": {
        "id": "RkyAbHwpP-Z6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQQDwfF4uIx6"
      },
      "source": [
        "#Feature importance analysis\n",
        "\n",
        "Pearson and Spearman correlation matrices used to study the correlation between each couple of features.\n",
        "\n",
        "Being all features quite highly correlated, I choose to keep just one of them (close price) to be able to use a window of more days in the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cjIkEjCuIx6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cac2f4dd-796c-4158-c73f-c8a0e484a68d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.99883087 0.99953588 0.99908664 0.73391176]\n",
            " [0.99883087 1.         0.99948583 0.99940123 0.73284666]\n",
            " [0.99953588 0.99948583 1.         0.99904118 0.73738097]\n",
            " [0.99908664 0.99940123 0.99904118 1.         0.72644921]\n",
            " [0.73391176 0.73284666 0.73738097 0.72644921 1.        ]]\n",
            "\n",
            "[[1.         0.99909409 0.99955709 0.99941132 0.93222606]\n",
            " [0.99909409 1.         0.99951155 0.99957452 0.93249602]\n",
            " [0.99955709 0.99951155 1.         0.9991788  0.93351421]\n",
            " [0.99941132 0.99957452 0.9991788  1.         0.9314874 ]\n",
            " [0.93222606 0.93249602 0.93351421 0.9314874  1.        ]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.stat import Correlation\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "def show_matrix(matrix):\n",
        "    \"\"\"\n",
        "    function to print a matrix on screen\n",
        "    \"\"\"\n",
        "    print(matrix.collect()[0][matrix.columns[0]].toArray())\n",
        "    print()\n",
        "\n",
        "vector_col = \"features\"\n",
        "assembler = VectorAssembler(inputCols=[\"Open\", \"Close\", \"High\", \"Low\", \"Volume\"], outputCol=vector_col)\n",
        "df_vector = assembler.transform(df).select(vector_col)\n",
        "\n",
        "matrix_pearson = Correlation.corr(df_vector, vector_col)    #pearson is default\n",
        "matrix_spearman = Correlation.corr(df_vector, vector_col, \"spearman\")\n",
        "show_matrix(matrix_pearson)\n",
        "show_matrix(matrix_spearman)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lalu lakukan perhitungan korelasi antara kolom-kolom dalam DataFrame PySpark."
      ],
      "metadata": {
        "id": "bIvB-QdVQStg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58_qdNd_uIx6"
      },
      "source": [
        "#Feature scaling\n",
        "\n",
        "Tanh estimator used to scale all the feature values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "n8quig5A6ziV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znxVYKlWuIx7"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "# Define the Tanh estimator function as a UDF\n",
        "@udf(returnType=FloatType())\n",
        "def tanh_estimator(x):\n",
        "    \"\"\"\n",
        "    User-defined function, applies Tanh estimator's formula to a feature value x\n",
        "    \"\"\"\n",
        "    return 0.5 * (math.tanh(0.01 * (x - mean) / std) + 1)\n",
        "\n",
        "# Define a function to scale the DataFrame using the Tanh estimator function\n",
        "def scale_transform(df):\n",
        "    \"\"\"\n",
        "    Transforms a DataFrame by applying the Tanh estimator UDF\n",
        "    \"\"\"\n",
        "    # Ensure \"Close\" column exists\n",
        "    if \"Close\" not in df.columns:\n",
        "        raise ValueError(\"Column 'Close' not found in DataFrame.\")\n",
        "\n",
        "    # Convert \"Close\" column to FloatType if it's not already\n",
        "    if df.schema[\"Close\"].dataType != FloatType():\n",
        "        df = df.withColumn(\"Close\", col(\"Close\").cast(FloatType()))\n",
        "\n",
        "    # Apply the Tanh estimator UDF to the \"Close\" column\n",
        "    return df.withColumn(\"Scaled_Close\", tanh_estimator(col(\"Close\")))\n",
        "\n",
        "# Apply scaling transformation to the training and test sets\n",
        "scaled_train_set = scale_transform(train_set)\n",
        "scaled_test_set = scale_transform(test_set)\n",
        "\n",
        "# Show the first few rows of scaled DataFrame\n",
        "scaled_train_set.show()\n",
        "scaled_test_set.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "mengaplikasikan transformasi skala pada DataFrame PySpark dengan menggunakan estimasi Tanh sebagai fungsi pemodelan skala"
      ],
      "metadata": {
        "id": "GjRVfNY-Ql3M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jutstO_KuIx8"
      },
      "source": [
        "#Sliding window\n",
        "\n",
        "Window of 30 days is slided on the close prices in order to create train and test set, composed by examples such as:\n",
        "\n",
        "x={day(i), ... , day(i+29)}, y={day(i+30)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWOQN36EuIx8"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "def slide_window(df, window_size):\n",
        "    \"\"\"\n",
        "    Returns two new dataframes:\n",
        "    X - obtained sliding a window of given size (=#window_size) on the original dataframe, aggregating #window_size close prices on the same row\n",
        "    y - for each row of X, y contains a row with the (single) price of the day after last day contained in X\n",
        "    \"\"\"\n",
        "\n",
        "    w = Window.orderBy(\"Date\")\n",
        "    indexed_df = df.withColumn(\"Index\", row_number().over(w)).select(\"Index\", \"Close\")    #adding index to be able to loop following order and create windows\n",
        "\n",
        "    schema = StructType([StructField(\"Close\", ArrayType(FloatType()), False)])   #schema for X (array of floats)\n",
        "\n",
        "    X = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema )\n",
        "    y = spark.createDataFrame(spark.sparkContext.emptyRDD(), FloatType())\n",
        "\n",
        "    length = indexed_df.count()\n",
        "    for i in range(window_size+1, length+1):\n",
        "        new_df = indexed_df.where(col(\"Index\").between(i-window_size, i-1)).select(\"Close\")    #select the window\n",
        "        new_row = new_df.agg(collect_list(\"Close\").alias(\"Close\"))    #create new X's row with all prices from window\n",
        "        X = X.union(new_row)\n",
        "        new_row = indexed_df.where(col(\"Index\") == i).select(\"Close\")    #create new Y's row with price of the day after last day contained in X\n",
        "        y = y.union(new_row)\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "kode diatas digunakan untuk membuat Sliding window pada DataFrame dengan mengumpulkan harga penutupan dari sejumlah hari yang diberikan dalam setiap window dan menghasilkan DataFrame baru untuk X (Sliding window) dan y (harga satu hari setelah window terakhir dalam X)."
      ],
      "metadata": {
        "id": "yqnZB_ApQ7jP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tWFY-2QuIx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58dec4ae-3853-4818-bcf6-4c5af3a6d60d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: StructType([StructField('Close', ArrayType(FloatType(), True), False)])\n",
            "y_train shape: StructType([StructField('value', FloatType(), True)])\n"
          ]
        }
      ],
      "source": [
        "window = 30    #window size\n",
        "\n",
        "X_train, y_train = slide_window(scaled_train_set, window)    #slide window on train set\n",
        "X_test, y_test = slide_window(scaled_test_set, window)    #slide window on test set\n",
        "\n",
        "print(\"X_train shape:\", X_train.schema)\n",
        "print(\"y_train shape:\", y_train.schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "kode tersebut digunakan untuk menerapkan Sliding window pada data pelatihan dan pengujian yang telah disesuaikan skala, dan kemudian menampilkan bentuk atau skema DataFrame yang dihasilkan setelah proses Sliding window"
      ],
      "metadata": {
        "id": "Fp6bX-s_RQD8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqHireO1uIx9"
      },
      "source": [
        "#Merging X and y\n",
        "\n",
        "X and y (for both train and test) need to be merged as the Pyspark regression models require them in a single dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17hIGQOEuIx9"
      },
      "outputs": [],
      "source": [
        "def merge_X_y(X, y):\n",
        "    \"\"\"\n",
        "    merges two dataframes column-wise\n",
        "    \"\"\"\n",
        "    schema = StructType(X.schema.fields + y.schema.fields)\n",
        "    X_y = X.rdd.zip(y.rdd).map(lambda x: x[0]+x[1])\n",
        "    return spark.createDataFrame(X_y, schema)\n",
        "\n",
        "X_y_train = merge_X_y(X_train, y_train)\n",
        "X_y_test = merge_X_y(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "menggabungkan dua DataFrame menjadi satu DataFrame dengan metode yang spesifik, di mana kolom-kolom dari kedua DataFrame digabungkan secara berdampingan"
      ],
      "metadata": {
        "id": "T4qJcZTJRt4h"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyK9y_1LuIx9"
      },
      "source": [
        "#Vectorization of windows\n",
        "\n",
        "Windows represented as lists of days need to be converted to vectors (rows) of features as Pyspark regression models require dataframes in this form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsFtALPyuIx9"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark import StorageLevel\n",
        "\n",
        "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())    #converts list of prices to features vector\n",
        "\n",
        "def assemble_window(X_y):\n",
        "    \"\"\"\n",
        "    applies list_to_vector_udf to given dataframe\n",
        "    \"\"\"\n",
        "    return X_y.select(list_to_vector_udf(X_y[\"Close\"]).alias(\"features\"), X_y[\"value\"].alias(\"label\"))\n",
        "\n",
        "X_y_train_vec = assemble_window(X_y_train)\n",
        "X_y_test_vec = assemble_window(X_y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "digunakan untuk mengonversi daftar harga dalam DataFrame menjadi vektor fitur dan menggabungkannya dengan label yang sesuai"
      ],
      "metadata": {
        "id": "s2DQdCP5SbI-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ODXg-uTuIx-"
      },
      "source": [
        "#Hyperparameter tuning/model selection/evaluation\n",
        "\n",
        "In this section linear regression and gradient-boosted trees regression models are cross-validated partitioning the train set in 3 folds in order to tune their hyperparameters and find the best model.\n",
        "\n",
        "Then the best models found are tested on unseen data (test set) and the actual and predicted prices are plotted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMTusFd8uIx-"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "def cross_validate(model, param_grid, df):\n",
        "    \"\"\"\n",
        "    Performs grid search on given model with given parameter grid, using given dataframe as train/validation data.\n",
        "    Returns the validated model (ready to be used for predictions using best parameters found)\n",
        "    \"\"\"\n",
        "    evaluator = RegressionEvaluator(metricName=\"rmse\")\n",
        "    cv = CrossValidator(estimator=model, estimatorParamMaps=param_grid, evaluator=evaluator)\n",
        "    validated_model = cv.fit(df)\n",
        "    return validated_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "digunakan untuk mengoptimalkan model pembelajaran mesin dengan melakukan pencarian grid dan validasi silang untuk menemukan set parameter terbaik, lalu mengembalikan model terbaik yang telah divalidasi."
      ],
      "metadata": {
        "id": "qu5XRjKJSooZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc_qyrFSuIx-"
      },
      "source": [
        "#Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wl7D6GtMuIx_"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "evaluator = RegressionEvaluator(metricName=\"rmse\")\n",
        "\n",
        "lr = LinearRegression(standardization=False)    #avoid standardization as tanh estimator has been applied yet\n",
        "param_grid = ParamGridBuilder().addGrid(lr.regParam, [0.33, 0.66]).addGrid(lr.elasticNetParam, [0.33, 0.5, 0.66]).build()    #parameters to be tuned\n",
        "validated_lr_model = cross_validate(lr, param_grid, X_y_train_vec)\n",
        "\n",
        "#best parameters found\n",
        "elasticNet = validated_lr_model.bestModel.getElasticNetParam()\n",
        "reg = validated_lr_model.bestModel.getRegParam()\n",
        "\n",
        "print(\"ElasicNetParam of best model -> \", elasticNet)\n",
        "print(\"RegParam of best model -> = \", reg)\n",
        "\n",
        "predictions = validated_lr_model.transform(X_y_test_vec)    #test on unseen data\n",
        "RMSE = evaluator.evaluate(predictions)    #evaluate predictions using ROOT MEAN SQUARED ERROR\n",
        "print(\"RMSE of best model on unseen data -> \", RMSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "kode diatas digunakan untuk mengoptimalkan model regresi linear dengan pencarian grid dan validasi silang, lalu mengevaluasi performanya pada data uji menggunakan metrik RMSE."
      ],
      "metadata": {
        "id": "8w2KIBc7S0jN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cU9o6dz4uIx_"
      },
      "outputs": [],
      "source": [
        "def plot_predictions(predictions):\n",
        "    \"\"\"\n",
        "    plots two lines representing predicted and actual prices\n",
        "    \"\"\"\n",
        "    pandas_df = predictions.select('label', 'prediction').toPandas()\n",
        "    plt.figure(figsize=(20, 7))\n",
        "    plt.plot(range(len(pandas_df['label'].values)), pandas_df['label'].values, label = 'Actual Price', color = 'blue')\n",
        "    plt.plot(range(len(pandas_df['prediction'].values)), pandas_df['prediction'].values, label = 'Predicted Price', color = 'red')\n",
        "    plt.xticks(np.arange(100, pandas_df.shape[0], 200))\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Price (scaled)')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_predictions(predictions)    #plot linear regression's predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZJfDMRTuIx_"
      },
      "source": [
        "#Gradient-boosted trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLYsqie5uIyA"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import GBTRegressor\n",
        "\n",
        "gbt_r = GBTRegressor()\n",
        "param_grid = ParamGridBuilder().addGrid(gbt_r.maxDepth, [4, 8, 12]).addGrid(gbt_r.featureSubsetStrategy, ['0.33', '0.66']).build()    #parameters to be tuned\n",
        "validated_gbt_model = cross_validate(gbt_r, param_grid, X_y_train_vec)\n",
        "\n",
        "#best parameters found\n",
        "max_depth = validated_gbt_model.bestModel.getMaxDepth()\n",
        "subsample = validated_gbt_model.bestModel.getFeatureSubsetStrategy()\n",
        "\n",
        "print(\"maxDepth of best model -> \", max_depth)\n",
        "print(\"featureSubsetStrategy of best model -> = \", subsample)\n",
        "\n",
        "predictions = validated_gbt_model.transform(X_y_test_vec)    #test on unseen data\n",
        "RMSE = evaluator.evaluate(predictions)    #evaluate predictions using ROOT MEAN SQUARED ERROR\n",
        "print(\"RMSE of best model on unseen data ->  \", RMSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPOXzYAMuIyB"
      },
      "outputs": [],
      "source": [
        "plot_predictions(predictions)    #plot gradient-boosted trees' predicitons"
      ]
    }
  ]
}