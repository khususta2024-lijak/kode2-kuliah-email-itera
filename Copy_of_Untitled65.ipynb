{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Code 3"],"metadata":{"id":"FZxI9WmbotZ3"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset"],"metadata":{"id":"efTem69To8V-","executionInfo":{"status":"ok","timestamp":1732113239290,"user_tz":-420,"elapsed":9845,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# Kosakata sederhana untuk contoh\n","src_vocab = {'How': 0, 'are': 1, 'you': 2, '?': 3, '<pad>': 4, '<sos>': 5, '<eos>': 6}\n","trg_vocab = {'comment': 0, 'vas': 1, 'tu': 2, '?': 3, '<pad>': 4, '<sos>': 5, '<eos>': 6}\n","\n","src_vocab_size = len(src_vocab)\n","trg_vocab_size = len(trg_vocab)\n","\n","# Membuat data dummy\n","src_data = torch.tensor([[5, 0, 1, 2, 3, 6]])  # Input: \"<sos> Nice to meet you <eos>\"\n","trg_data = torch.tensor([[5, 0, 1, 2, 3, 6]])  # Target: \"<sos> ravi de vous rencontrer <eos>\"\n","\n","# Dataset dan DataLoader\n","train_dataset = TensorDataset(src_data, trg_data)\n","train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)"],"metadata":{"id":"E7Zc2pJUovCE","executionInfo":{"status":"ok","timestamp":1732113239291,"user_tz":-420,"elapsed":4,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Model Encoder-Decoder\n","class Encoder(nn.Module):\n","    def __init__(self, input_dim, hidden_dim):\n","        super(Encoder, self).__init__()\n","        self.embedding = nn.Embedding(input_dim, hidden_dim)\n","        self.rnn = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        outputs, (hidden, cell) = self.rnn(embedded)\n","        return hidden, cell\n","\n","class Decoder(nn.Module):\n","    def __init__(self, output_dim, hidden_dim):\n","        super(Decoder, self).__init__()\n","        self.embedding = nn.Embedding(output_dim, hidden_dim)\n","        self.rnn = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x, hidden, cell):\n","        x = x.unsqueeze(1)\n","        embedded = self.embedding(x)\n","        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n","        prediction = self.fc(output.squeeze(1))\n","        return prediction, hidden, cell\n","\n","class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n","        batch_size = trg.shape[0]\n","        trg_len = trg.shape[1]\n","        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size)\n","\n","        hidden, cell = self.encoder(src)\n","\n","        input_token = trg[:, 0]\n","\n","        for t in range(1, trg_len):\n","            output, hidden, cell = self.decoder(input_token, hidden, cell)\n","            outputs[:, t] = output\n","\n","            top1 = output.argmax(1)\n","            input_token = trg[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n","\n","        return outputs"],"metadata":{"id":"dOMXAAlzoyvw","executionInfo":{"status":"ok","timestamp":1732113239291,"user_tz":-420,"elapsed":3,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Kode yang dibangun mendefinisikan model **Encoder-Decoder** dengan LSTM untuk tugas pemrosesan sekuensial seperti penerjemahan sebuah kata dengan data yang telah ddi inputkan. **Encoder** memproses input sekuensial melalui embedding dan LSTM untuk menghasilkan hidden state dan cell state. **Decoder** menerima state ini dan menghasilkan prediksi token satu per satu, menggunakan embedding, LSTM, dan layer linear untuk memetakan ke ruang output.\n","\n","Kelas **Seq2Seq** menggabungkan encoder dan decoder, dan menggunakan **teacher forcing** untuk menentukan apakah input berikutnya ke decoder adalah token target sebenarnya atau prediksi sebelumnya. Hasil akhirnya adalah tensor prediksi urutan output. Performa model dapat ditingkatkan dengan penambahan **dropout**, **encoder bidirectional**, atau **attention mechanism** untuk fokus yang lebih baik pada input saat decoding."],"metadata":{"id":"td0dq-2VubSt"}},{"cell_type":"code","source":["# Hyperparameters\n","hidden_dim = 256\n","encoder = Encoder(src_vocab_size, hidden_dim)\n","decoder = Decoder(trg_vocab_size, hidden_dim)\n","model = Seq2Seq(encoder, decoder)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","num_epochs = 100\n","for epoch in range(num_epochs):\n","    model.train()\n","    for src_batch, trg_batch in train_loader:\n","        optimizer.zero_grad()\n","        output = model(src_batch, trg_batch)\n","        loss = criterion(output.view(-1, trg_vocab_size), trg_batch.view(-1))\n","        loss.backward()\n","        optimizer.step()\n","\n","    if (epoch + 1) % 10 == 0:\n","        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uzr-8CYDo2Tt","outputId":"598c68c4-a36d-4f7f-fdea-a495cf563a2a","executionInfo":{"status":"ok","timestamp":1732113253695,"user_tz":-420,"elapsed":14407,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 10/100, Loss: 0.49359074234962463\n","Epoch 20/100, Loss: 0.33720406889915466\n","Epoch 30/100, Loss: 0.32823798060417175\n","Epoch 40/100, Loss: 0.3265061676502228\n","Epoch 50/100, Loss: 0.32591694593429565\n","Epoch 60/100, Loss: 0.3256341218948364\n","Epoch 70/100, Loss: 0.32546094059944153\n","Epoch 80/100, Loss: 0.32533586025238037\n","Epoch 90/100, Loss: 0.32523614168167114\n","Epoch 100/100, Loss: 0.32515260577201843\n"]}]},{"cell_type":"code","source":["# Function untuk menerjemahkan kalimat\n","def translate_sentence(model, sentence, src_vocab, trg_vocab):\n","    model.eval()\n","    tokens = [src_vocab[token] for token in sentence.split()] + [src_vocab['<eos>']]\n","    src_tensor = torch.tensor([tokens])\n","\n","    with torch.no_grad():\n","        hidden, cell = model.encoder(src_tensor)\n","        trg_indexes = [trg_vocab['<sos>']]\n","\n","        for _ in range(10):  # Batas panjang kalimat hasil\n","            trg_tensor = torch.tensor([trg_indexes[-1]])\n","            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n","            pred_token = output.argmax(1).item()\n","            trg_indexes.append(pred_token)\n","\n","            if pred_token == trg_vocab['<eos>']:\n","                break\n","\n","    translated_sentence = [list(trg_vocab.keys())[list(trg_vocab.values()).index(idx)] for idx in trg_indexes[1:-1]]\n","    return ' '.join(translated_sentence)\n","\n","# Contoh penggunaan\n","translated_output = translate_sentence(model, \"Nice to meet you\", src_vocab, trg_vocab)\n","print(\"Terjemahan:\", translated_output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"I1LxUThCo5ZH","outputId":"abb9ea34-a0a6-4013-ae0a-81bf28fa8061","executionInfo":{"status":"error","timestamp":1732113302983,"user_tz":-420,"elapsed":487,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"execution_count":6,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'Nice'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-f169190e16f5>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Contoh penggunaan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtranslated_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Nice to meet you\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Terjemahan:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslated_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-f169190e16f5>\u001b[0m in \u001b[0;36mtranslate_sentence\u001b[0;34m(model, sentence, src_vocab, trg_vocab)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msrc_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msrc_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0msrc_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-f169190e16f5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msrc_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msrc_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0msrc_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'Nice'"]}]},{"cell_type":"markdown","source":["# Nomor 1"],"metadata":{"id":"LjvCDoDKpd3n"}},{"cell_type":"code","source":["optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n","\n","for epoch in range(num_epochs):\n","    train_loss = 0\n","    model.train()\n","    for src_batch, trg_batch in train_loader:\n","        optimizer.zero_grad()\n","        output = model(src_batch, trg_batch)\n","        loss = criterion(output.view(-1, trg_vocab_size), trg_batch.view(-1))\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()\n","\n","    scheduler.step(train_loss)\n","    print(f'Epoch {epoch + 1}, Loss: {train_loss}')"],"metadata":{"id":"kZYXzUN_pdl7","executionInfo":{"status":"aborted","timestamp":1732113253695,"user_tz":-420,"elapsed":6,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Nomor 2"],"metadata":{"id":"XfCm-gPjp_Ya"}},{"cell_type":"code","source":["# Model Encoder-Decoder dengan dropout opsional\n","class Encoder(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, dropout=0.0):\n","        super(Encoder, self).__init__()\n","        self.embedding = nn.Embedding(input_dim, hidden_dim)\n","        self.rnn = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        embedded = self.dropout(self.embedding(x))\n","        outputs, (hidden, cell) = self.rnn(embedded)\n","        return hidden, cell\n","\n","class Decoder(nn.Module):\n","    def __init__(self, output_dim, hidden_dim, dropout=0.0):\n","        super(Decoder, self).__init__()\n","        self.embedding = nn.Embedding(output_dim, hidden_dim)\n","        self.rnn = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, hidden, cell):\n","        x = x.unsqueeze(1)\n","        embedded = self.dropout(self.embedding(x))\n","        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n","        prediction = self.fc(output.squeeze(1))\n","        return prediction, hidden, cell\n","\n","class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n","        batch_size = trg.shape[0]\n","        trg_len = trg.shape[1]\n","        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size)\n","\n","        hidden, cell = self.encoder(src)\n","\n","        input_token = trg[:, 0]\n","\n","        for t in range(1, trg_len):\n","            output, hidden, cell = self.decoder(input_token, hidden, cell)\n","            outputs[:, t] = output\n","\n","            top1 = output.argmax(1)\n","            input_token = trg[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n","\n","        return outputs\n","\n","# Hyperparameters\n","hidden_dim = 256\n","dropout_rate = 0.3  # Dropout rate untuk eksperimen\n","encoder_with_dropout = Encoder(src_vocab_size, hidden_dim, dropout=dropout_rate)\n","decoder_with_dropout = Decoder(trg_vocab_size, hidden_dim, dropout=dropout_rate)\n","model_with_dropout = Seq2Seq(encoder_with_dropout, decoder_with_dropout)\n","\n","encoder_without_dropout = Encoder(src_vocab_size, hidden_dim)\n","decoder_without_dropout = Decoder(trg_vocab_size, hidden_dim)\n","model_without_dropout = Seq2Seq(encoder_without_dropout, decoder_without_dropout)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer_with_dropout = optim.Adam(model_with_dropout.parameters(), lr=0.001)\n","optimizer_without_dropout = optim.Adam(model_without_dropout.parameters(), lr=0.001)\n","\n","# Training function\n","def train_model(model, optimizer, num_epochs=100):\n","    for epoch in range(num_epochs):\n","        model.train()\n","        for src_batch, trg_batch in train_loader:\n","            optimizer.zero_grad()\n","            output = model(src_batch, trg_batch)\n","            loss = criterion(output.view(-1, trg_vocab_size), trg_batch.view(-1))\n","            loss.backward()\n","            optimizer.step()\n","\n","        if (epoch + 1) % 10 == 0:\n","            print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n","\n","# Eksperimen dengan dropout\n","print(\"Training with dropout...\")\n","train_model(model_with_dropout, optimizer_with_dropout)\n","\n","# Eksperimen tanpa dropout\n","print(\"\\nTraining without dropout...\")\n","train_model(model_without_dropout, optimizer_without_dropout)"],"metadata":{"id":"LvyciL3pp9MB","executionInfo":{"status":"aborted","timestamp":1732113253695,"user_tz":-420,"elapsed":6,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Nomor 3"],"metadata":{"id":"nJuhtpkNqrPr"}},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers=1, bidirectional=True):\n","        super(Encoder, self).__init__()\n","        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers, bidirectional=bidirectional, batch_first=True)\n","        self.hidden_dim = hidden_dim\n","        self.num_directions = 2 if bidirectional else 1\n","\n","    def forward(self, x):\n","        outputs, (hidden, cell) = self.rnn(x)\n","        if self.num_directions == 2:  # Jika bidirectional\n","            hidden = torch.cat((hidden[-2], hidden[-1]), dim=1).unsqueeze(0)\n","            cell = torch.cat((cell[-2], cell[-1]), dim=1).unsqueeze(0)\n","        return hidden, cell\n","\n","class Decoder(nn.Module):\n","    def __init__(self, output_dim, hidden_dim):\n","        super(Decoder, self).__init__()\n","        self.rnn = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x, hidden, cell):\n","        x = x.unsqueeze(1)\n","        output, (hidden, cell) = self.rnn(x, (hidden, cell))\n","        prediction = self.fc(output.squeeze(1))\n","        return prediction, hidden, cell"],"metadata":{"id":"JVrVUf2srGA3","executionInfo":{"status":"aborted","timestamp":1732113253695,"user_tz":-420,"elapsed":5,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Nomor 4"],"metadata":{"id":"99eBWVoJrKLi"}},{"cell_type":"code","source":["# Fungsi pelatihan dengan scheduled sampling\n","def train_with_scheduled_sampling(model, train_loader, optimizer, criterion, trg_vocab_size, num_epochs=10, scheduled_sampling_ratio=0.5):\n","    for epoch in range(num_epochs):\n","        model.train()\n","        epoch_loss = 0\n","\n","        for src_batch, trg_batch in train_loader:\n","            optimizer.zero_grad()\n","            output = model(src_batch, trg_batch, scheduled_sampling_ratio)\n","            loss = criterion(output.view(-1, trg_vocab_size), trg_batch.view(-1))\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","        # Kurangi scheduled_sampling_ratio secara bertahap jika diinginkan\n","        if epoch % 5 == 0 and scheduled_sampling_ratio < 1.0:\n","            scheduled_sampling_ratio += 0.05\n","\n","        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_loader)}')\n","\n","# Penyesuaian pada metode forward model\n","class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, src, trg, scheduled_sampling_ratio=0.5):\n","        batch_size = trg.shape[0]\n","        trg_len = trg.shape[1]\n","        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(src.device)\n","\n","        hidden, cell = self.encoder(src)\n","        input_token = trg[:, 0]\n","\n","        for t in range(1, trg_len):\n","            output, hidden, cell = self.decoder(input_token, hidden, cell)\n","            outputs[:, t] = output\n","\n","            top1 = output.argmax(1)\n","            use_teacher_forcing = random.random() < scheduled_sampling_ratio\n","            input_token = trg[:, t] if use_teacher_forcing else top1\n","\n","        return outputs\n","\n","# Panggil fungsi pelatihan dengan scheduled sampling\n","train_with_scheduled_sampling(model_with_dropout, train_loader, optimizer_with_dropout, criterion, trg_vocab_size, num_epochs=100)"],"metadata":{"id":"aEv0Uv2XrMEc","executionInfo":{"status":"aborted","timestamp":1732113253695,"user_tz":-420,"elapsed":5,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Nomor 5"],"metadata":{"id":"KHYAW5EirkBO"}},{"cell_type":"code","source":["# Encoder menggunakan GRU\n","class EncoderGRU(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers=1, bidirectional=False):\n","        super(EncoderGRU, self).__init__()\n","        self.embedding = nn.Embedding(input_dim, hidden_dim)\n","        self.rnn = nn.GRU(hidden_dim, hidden_dim, num_layers, bidirectional=bidirectional, batch_first=True)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        outputs, hidden = self.rnn(embedded)\n","        if self.rnn.bidirectional:\n","            hidden = torch.cat((hidden[-2], hidden[-1]), dim=1).unsqueeze(0)\n","        return hidden\n","\n","# Decoder menggunakan GRU\n","class DecoderGRU(nn.Module):\n","    def __init__(self, output_dim, hidden_dim, num_layers=1):\n","        super(DecoderGRU, self).__init__()\n","        self.embedding = nn.Embedding(output_dim, hidden_dim)\n","        self.rnn = nn.GRU(hidden_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x, hidden):\n","        x = x.unsqueeze(1)\n","        embedded = self.embedding(x)\n","        output, hidden = self.rnn(embedded, hidden)\n","        prediction = self.fc(output.squeeze(1))\n","        return prediction, hidden\n","\n","# Model Seq2Seq dengan GRU\n","class Seq2SeqGRU(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(Seq2SeqGRU, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n","        batch_size = trg.shape[0]\n","        trg_len = trg.shape[1]\n","        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(src.device)\n","\n","        hidden = self.encoder(src)\n","        input_token = trg[:, 0]\n","\n","        for t in range(1, trg_len):\n","            output, hidden = self.decoder(input_token, hidden)\n","            outputs[:, t] = output\n","            top1 = output.argmax(1)\n","            input_token = trg[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n","\n","        return outputs\n","\n","# Definisi hyperparameters dan model\n","hidden_dim = 256\n","encoder_gru = EncoderGRU(src_vocab_size, hidden_dim)\n","decoder_gru = DecoderGRU(trg_vocab_size, hidden_dim)\n","model_gru = Seq2SeqGRU(encoder_gru, decoder_gru)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer_gru = optim.Adam(model_gru.parameters(), lr=0.001)\n","\n","# Fungsi pelatihan sama seperti sebelumnya\n","def train_model(model, optimizer, num_epochs=10):\n","    for epoch in range(num_epochs):\n","        model.train()\n","        for src_batch, trg_batch in train_loader:\n","            optimizer.zero_grad()\n","            output = model(src_batch, trg_batch)\n","            loss = criterion(output.view(-1, trg_vocab_size), trg_batch.view(-1))\n","            loss.backward()\n","            optimizer.step()\n","\n","        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n","\n","# Latihan model dengan GRU\n","train_model(model_gru, optimizer_gru)\n"],"metadata":{"id":"i5KXbDEdrjqv","executionInfo":{"status":"aborted","timestamp":1732113253696,"user_tz":-420,"elapsed":6,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Nomor 6"],"metadata":{"id":"tRn43KAdsPIB"}},{"cell_type":"code","source":["from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","import torch\n","\n","# Fungsi untuk melakukan prediksi dengan model\n","def translate(model, src_sentence, src_vocab, trg_vocab, max_len=20):\n","    model.eval()\n","    tokens = [src_vocab[token] for token in src_sentence.split()] + [src_vocab['<eos>']]\n","    src_tensor = torch.tensor([tokens]).to('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    with torch.no_grad():\n","        hidden = model.encoder(src_tensor)\n","        trg_indexes = [trg_vocab['<sos>']]\n","\n","        for _ in range(max_len):\n","            trg_tensor = torch.tensor([trg_indexes[-1]]).to('cuda' if torch.cuda.is_available() else 'cpu')\n","            output, hidden = model.decoder(trg_tensor, hidden)\n","            pred_token = output.argmax(1).item()\n","            trg_indexes.append(pred_token)\n","\n","            if pred_token == trg_vocab['<eos>']:\n","                break\n","\n","    translated_sentence = [list(trg_vocab.keys())[list(trg_vocab.values()).index(idx)] for idx in trg_indexes[1:-1]]\n","    return ' '.join(translated_sentence)\n","\n","# Evaluasi skor BLEU\n","def evaluate_bleu(model, src_sentences, trg_sentences, src_vocab, trg_vocab):\n","    bleu_scores = []\n","    for i, src_sentence in enumerate(src_sentences):\n","        reference = [trg_sentences[i].split()]  # Referensi target\n","        candidate = translate(model, src_sentence, src_vocab, trg_vocab).split()  # Prediksi model\n","        score = sentence_bleu(reference, candidate, smoothing_function=SmoothingFunction().method1)\n","        bleu_scores.append(score)\n","        print(f'Sentence {i + 1}: BLEU score = {score:.4f}')\n","    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n","    print(f'\\nAverage BLEU score: {avg_bleu:.4f}')\n","    return avg_bleu\n","\n","# Contoh data untuk evaluasi\n","src_sentences = [\"How are you?\"]  # Kalimat input\n","trg_sentences = [\"comment vas tu?\"]  # Terjemahan referensi\n","\n","# Evaluasi model dengan skor BLEU\n","average_bleu = evaluate_bleu(model_gru, src_sentences, trg_sentences, src_vocab, trg_vocab)\n"],"metadata":{"id":"BVKPYX4Rsfz5","executionInfo":{"status":"aborted","timestamp":1732113253696,"user_tz":-420,"elapsed":6,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"execution_count":null,"outputs":[]}]}