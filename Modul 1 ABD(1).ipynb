{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tujuan Praktikum\n",
    "\n",
    "1. Mahasiswa dapat menginstal Hadoop 3\n",
    "\n",
    "2. Mahasiswa dapat memahami konsep HDFS, MapReduce, dan Yarn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Distributed File System\n",
    "\n",
    "HDFS adalah sistem file berbasis perangkat lunak yang diimplementasikan dalam Java dan berada di atas sistem file asli. Konsep utama di balik HDFS adalah membagi file menjadi blok (biasanya 128 MB) alih-alih menangani file secara keseluruhan. Hal ini memungkinkan banyak fitur seperti distribusi, replikasi, pemulihan kegagalan, dan yang lebih penting adalah pemrosesan terdistribusi blok menggunakan beberapa mesin. Ukuran blok bisa 64 MB, 128 MB, 256 MB, atau 512 MB, tergantung pada kebutuhan. Untuk file 1 GB dengan blok 128 MB, akan ada 1024 MB/128 MB sama dengan delapan blok. Jika Anda mempertimbangkan faktor replikasi tiga, ini menjadikannya 24 blok. HDFS menyediakan sistem penyimpanan terdistribusi dengan toleransi kesalahan dan pemulihan kegagalan. HDFS memiliki dua komponen utama: NameNode dan DataNode.\n",
    "NameNode berisi semua metadata dari semua konten sistem file: nama file, izin file, dan lokasi setiap blok setiap file, dan oleh karena itu merupakan mesin paling penting di HDFS. DataNode terhubung ke NameNode dan menyimpan blok dalam HDFS. Mereka bergantung pada NameNode untuk semua informasi metadata mengenai konten dalam sistem file. Jika NameNode tidak memiliki informasi apa pun, DataNode tidak akan dapat memberikan informasi kepada klien mana pun yang ingin membaca/menulis ke HDFS.\n",
    "Dimungkinkan untuk menjalankan proses NameNode dan DataNode pada satu mesin; namun, umumnya kluster HDFS terdiri dari server khusus yang menjalankan proses NameNode dan ribuan mesin yang menjalankan proses DataNode. Agar dapat mengakses informasi konten yang disimpan di NameNode, ia menyimpan seluruh struktur metadata dalam memori. Ini memastikan bahwa tidak ada kehilangan data sebagai akibat dari kegagalan mesin dengan melacak faktor replikasi blok. Karena ini adalah titik kegagalan tunggal, untuk mengurangi risiko kehilangan data akibat kegagalan NameNode, NameNode sekunder dapat digunakan untuk menghasilkan snapshot dari struktur memori NameNode utama.\n",
    "DataNode memiliki kapasitas penyimpanan yang besar dan, tidak seperti NameNode, HDFS akan terus beroperasi normal jika DataNode gagal. Ketika DataNode gagal, NameNode secara otomatis menangani replikasi yang berkurang dari semua blok data di DataNode yang gagal dan memastikan replikasi dibangun kembali. Karena NameNode mengetahui semua lokasi blok yang direplikasi, klien mana pun yang terhubung ke kluster dapat melanjutkan dengan sedikit atau tanpa hambatan.\n",
    "\n",
    "1. **High Availability (HA):**\n",
    "   High Availability dalam konteks HDFS adalah kemampuan sistem untuk tetap beroperasi dan menyediakan akses ke data meskipun terjadi kegagalan pada beberapa komponen. Dalam HDFS, HA dicapai dengan menggunakan konfigurasi Active/Standby NameNode. Jika NameNode aktif mengalami kegagalan, salah satu NameNode cadangan akan mengambil alih perannya secara otomatis, memastikan bahwa sistem tetap dapat diakses dan data tetap tersedia.\n",
    "\n",
    "2. **Intra-DataNode Balancer:**\n",
    "   Intra-DataNode Balancer adalah fitur di HDFS yang bertujuan untuk mengoptimalkan distribusi ruang penyimpanan di dalam satu DataNode. Ketika ruang penyimpanan pada beberapa disk dalam satu DataNode tidak seimbang, balancer ini akan secara otomatis memindahkan blok data antar disk untuk mencapai distribusi yang lebih seimbang. Hal ini meningkatkan efisiensi dan kinerja penyimpanan dalam cluster HDFS.\n",
    "\n",
    "3. **Erasure Coding (EC):**\n",
    "   Erasure Coding adalah teknik untuk melindungi data dari kegagalan disk dengan cara yang lebih efisien dibandingkan replikasi tradisional. Dalam HDFS, EC memungkinkan data disimpan dalam bentuk blok yang terenkripsi secara matematis. Ketika terjadi kegagalan disk, data yang hilang dapat direkonstruksi dari blok yang tersisa. EC mengurangi jumlah ruang penyimpanan yang dibutuhkan untuk melindungi data dibandingkan dengan replikasi tiga kali lipat, yang umum digunakan di HDFS.\n",
    "\n",
    "4. **Port Mapping:**\n",
    "   Port mapping adalah konfigurasi yang menentukan port mana yang digunakan oleh berbagai komponen HDFS untuk berkomunikasi. Misalnya, port default untuk NameNode adalah 8020, sedangkan untuk DataNode adalah 50010 untuk transfer data dan 50020 untuk transfer metadata. Dalam praktikum, penting untuk memahami port mapping agar mahasiswa dapat mengkonfigurasi firewall dan jaringan dengan benar, serta untuk melakukan troubleshooting jika terjadi masalah komunikasi antar komponen HDFS.\n",
    "   NameNode ports: 50470 -> 9871, 50070 -> 9870, dan 8020 -> 9820\n",
    "   Secondary NameNode ports: 50091 -> 9869 dan 50090 -> 9868\n",
    "   DataNode ports: 50020 -> 9867, 50010 -> 9866, 50475 -> 9865, dan 50075 -> 9864"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Hadoop on Windows 10 machines\n",
    "\n",
    "## Required tools\n",
    "1. Java JDK - used to run the Hadoop since it's built using Java\n",
    "2. 7Zip or WinRAR - unzip Hadoop binary package; anything that unzips `tar.gz`\n",
    "3. CMD or Powershell - used to test environment variables and run Hadoop\n",
    "\n",
    "## Step 1 - Download and extract Hadoop\n",
    "Download Hadoop from their official website and unzip the file. We'll be using [Hadoop 3.2.1](https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz). Hadoop is portable so you can store it on an external hard drive. For the purpose of documentation, I will extract it to `C:/Users/Anthony/Documents/cp-master`.\n",
    "\n",
    "If there are permission errors, run your unzipping program as administrator and unzip again.\n",
    "\n",
    "## Step 2 - Install Hadoop native IO binary\n",
    "Clone or download the [`winutils` repository](https://github.com/cdarlint/winutils) and copy the contents of `hadoop-3.2.1/bin` into the extracted location of the Hadoop binary package. In our example, it will be `C:\\Users\\Anthony\\Documents\\cp-master\\hadoop-3.2.1\\bin`\n",
    "\n",
    "## Step 3 - Install Java JDK\n",
    "Java JDK is required to run Hadoop, so if you haven't installed it, install it.\n",
    "\n",
    "Oracle requires you sign up and login to download it. I suggest you find an alternative resource to download it from [for example here (JDK 8u261)](https://enos.itcollege.ee/~jpoial/allalaadimised/jdk8/). This resource might not exist forever, so Google 'jdk version download'.\n",
    "\n",
    "Run the installation file and the default installation directory will be `C:\\Program Files\\Java\\jdk1.8.0_261`. \n",
    "\n",
    "After installation, open up CMD or Powershell and confirm Java is intalled:\n",
    "```\n",
    "$ java -version\n",
    "java version \"1.8.0_261\"\n",
    "Java(TM) SE Runtime Environment (build 1.8.0_261-b12)\n",
    "Java HotSpot(TM) 64-Bit Server VM (build 25.261-b12, mixed mode)\n",
    "```\n",
    "\n",
    "## Step 4 - Configure environment variables\n",
    "\n",
    "Open the Start Menu and type in 'environment' and press enter. A new window with System Properties should open up. Click the `Environment Variables` button near the bottom right.\n",
    "\n",
    "### `JAVA_HOME` environment variable\n",
    "1. From step 3, find the location of where you installed Java. In this example, the default directory is `C:\\Program Files\\Java\\jdk1.8.0_261`\n",
    "2. Create a new **User variable** with the variable name as `JAVA_HOME` and the value as `C:\\Program Files\\Java\\jdk1.8.0_261`\n",
    "\n",
    "### `HADOOP_HOME` environment variable\n",
    "1. From step 1, copy the directory you extracted the Hadoop binaries to. In this example, the directory is `C:\\Users\\Anthony\\Documents\\cp-master\\hadoop-3.2.1`\n",
    "2. Create a new **User variable** with the variable name as `HADOOP_HOME` and the value as `C:\\Users\\Anthony\\Documents\\cp-master\\hadoop-3.2.1`\n",
    "\n",
    "### `PATH` environment variable \n",
    "We'll now need to add the bin folders to the `PATH` environment variable.\n",
    "1. Click `Path` then `Edit`\n",
    "2. Click `New` on the top right\n",
    "3. Add `C:\\Users\\Anthony\\Documents\\cp-master\\hadoop-3.2.1\\bin` \n",
    "4. Add `C:\\Program Files\\Java\\jdk1.8.0_261\\bin`\n",
    "\n",
    "### Hadoop environment\n",
    "Hadoop complains about the directory if the `JAVA_HOME` directory has spaces. In the default installation directory, `Program Files` has a space which is problematic. To fix this, open the `%HADOOP_HOME%\\etc\\hadoop\\hadoop-env.cmd` and change the `JAVA_HOME` line to the following:\n",
    "```\n",
    "set JAVA_HOME=C:\\PROGRA~1\\Java\\jdk1.8.0_261\n",
    "```\n",
    "\n",
    "After setting those environment variables, you reopen CMD or Powershell and verify that the `hadoop` command is available:\n",
    "```\n",
    "$ hadoop -version\n",
    "java version \"1.8.0_261\"\n",
    "Java(TM) SE Runtime Environment (build 1.8.0_261-b12)\n",
    "Java HotSpot(TM) 64-Bit Server VM (build 25.261-b12, mixed mode)\n",
    "```\n",
    "\n",
    "## Step 5 - Configure Hadoop\n",
    "Now we are ready to configure the **most important part** - Hadoop configurations which involves Core, YARN, MapReduce, and HDFS configurations. \n",
    "\n",
    "Each of the files are in `%HADOOP_HOME%\\etc\\hadoop`. The full path for this example is `C:\\Users\\Anthony\\Documents\\cp-master\\hadoop-3.2.1\\etc\\hadoop`\n",
    "\n",
    "### Configure core site\n",
    "Edit `core-site.xml` and replace the `configuration` element with the following:\n",
    "```\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>fs.default.name</name>\n",
    "    <value>hdfs://0.0.0.0:19000</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "### Configure HDFS\n",
    "Create two folders, one for the namenode directory and another for the data directory. The following are the two created folders in this example:\n",
    "\n",
    "1. `C:\\Users\\Anthony\\Documents\\cp-master\\hadoop-3.2.1\\data\\dfs\\namespace_logs`\n",
    "2. `C:\\Users\\Anthony\\Documents\\cp-master\\hadoop-3.2.1\\data\\dfs\\data`\n",
    "\n",
    "Edit `hdfs-site.xml` and replace the `configuration` element with the following:\n",
    "```\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>dfs.replication</name>\n",
    "    <value>1</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.namenode.name.dir</name>\n",
    "    <!-- <value>file:///DIRECTORY 1 HERE</value> -->\n",
    "    <value>file:///C:/Users/Anthony/Documents/cp-master/hadoop-3.2.1/data/dfs/namespace_logs</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.datanode.data.dir</name>\n",
    "    <!-- <value>file:///DIRECTORY 2 HERE</value> -->\n",
    "    <value>file:///C:/Users/Anthony/Documents/cp-master/hadoop-3.2.1/data/dfs/data</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "### Configure MapReduce and YARN site\n",
    "Edit `mapred-site.xml` and replace the `configuration` element with the following:\n",
    "```\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>mapreduce.framework.name</name>\n",
    "    <value>yarn</value>\n",
    "  </property>\n",
    "  <property> \n",
    "    <name>mapreduce.application.classpath</name>\n",
    "    <value>%HADOOP_HOME%/share/hadoop/mapreduce/*,%HADOOP_HOME%/share/hadoop/mapreduce/lib/*,%HADOOP_HOME%/share/hadoop/common/*,%HADOOP_HOME%/share/hadoop/common/lib/*,%HADOOP_HOME%/share/hadoop/yarn/*,%HADOOP_HOME%/share/hadoop/yarn/lib/*,%HADOOP_HOME%/share/hadoop/hdfs/*,%HADOOP_HOME%/share/hadoop/hdfs/lib/*</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "Edit `yarn-site.xml` and replace the `configuration` element with the following:\n",
    "```\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>yarn.resourcemanager.hostname</name>\n",
    "    <value>localhost</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.aux-services</name>\n",
    "    <value>mapreduce_shuffle</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.env-whitelist</name>\n",
    "    <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "## Step 6 - Initialize HDFS and bugfix\n",
    "Run the following command and you should find the following error:\n",
    "```\n",
    "$ hdfs namenode -format\n",
    "...\n",
    "ERROR namenode.NameNode: Failed to start namenode.\n",
    "...\n",
    "```\n",
    "To fix this, you'll need to [download the a JAR file with the fix](https://github.com/FahaoTang/big-data/blob/master/hadoop-hdfs-3.2.1.jar). Overwrite the existing `hadoop-hdfs-3.2.1.jar` in `%HADOOP_HOME%\\share\\hadoop\\hdfs` with this new JAR file (you can make a backup of the current one before overwriting if you wish). \n",
    "\n",
    "## Step 7 - Start HDFS daemons\n",
    "Run the following command to start HDFS daemons. When you do so, there should be two new windows that open: one for datanode and the other for namenode:\n",
    "```\n",
    "$ %HADOOP_HOME%\\sbin\\start-dfs.cmd\n",
    "```\n",
    "\n",
    "## Step 8 - Start YARN daemons\n",
    "You might encounter permission issues as a normal user, so open a command line with elevated permissions. **If you have the [`yarn`](https://yarnpkg.com/) package manager, you will NOT be able to run YARN daemons since both use the `yarn` command.** To fix this, you must uninstall `yarn` package manager. \n",
    "\n",
    "Run the following command (with elevated permissions) to start YARN daemons. When you do so, there should be two new windows that open: one for resource manager and the other for node manager:\n",
    "```\n",
    "$ %HADOOP_HOME%\\sbin\\start-yarn.cmd\n",
    "```\n",
    "\n",
    "## Step 9 - Useful Web portals\n",
    "The daemons also host websites that provide useful information about the cluster\n",
    "\n",
    "### HDFS Namenode UI info\n",
    "[http://localhost:9870/dfshealth.html#tab-overview](http://localhost:9870/dfshealth.html#tab-overview)\n",
    "\n",
    "### HDFS Datanode UI info\n",
    "[http://localhost:9864/datanode.html](http://localhost:9864/datanode.html)\n",
    "\n",
    "### YARN resource manager UI\n",
    "[http://localhost:8088](http://localhost:8088)\n",
    "\n",
    "## Step 10 - Shutdown YARN and HDFS daemons\n",
    "You can stop the daemons by running the following commands:\n",
    "```\n",
    "$ %HADOOP_HOME%\\sbin\\stop-dfs.cmd\n",
    "$ %HADOOP_HOME%\\sbin\\stop-yarn.cmd\n",
    "```\n",
    "\n",
    "# Running MapReduce Jobs\n",
    "After setting up your environment and running the HDFS and YARN daemons, we can start working on running MapReduce jobs on our local machine. We need to compile our code, produce a JAR file, move our inputs, and run a MapReduce job on Hadoop.\n",
    "\n",
    "## Step 1 - Configure extra environment variables\n",
    "As a preface, it is best to setup some extra environment variables to make running jobs from the CLI quicker and easier. You can name these environment variables anything you want, but we will name them `HADOOP_CP` and `HDFS_LOC` to not potentially conlict with other environment variables.\n",
    "\n",
    "Open the Start Menu and type in 'environment' and press enter. A new window with System Properties should open up. Click the `Environment Variables` button near the bottom right.\n",
    "\n",
    "### `HADOOP_CP` environment variable\n",
    "This is used to compile your Java files. The backticks (eg. \\``some command here`\\`) do not work on Windows so we need to create a new environment variable with the results. If you need to add more packages, be sure to update the `HADOOP_CP` environment variable.\n",
    "\n",
    "1. Open a CLI and type in `hadoop classpath`. This will produce all the locations to the Hadoop libraries required to compile code, so copy all of this\n",
    "2. Create a new **User variable** with the variable name as `HADOOP_CP` and the value as the results of `hadoop classpath` command\n",
    "\n",
    "### `HDFS_LOC` environment variable\n",
    "This is used to reference the HDFS without having to constantly type the reference\n",
    "\n",
    "1. Create a new **User variable** with the variable name as `HDFS_LOC` and the value as `hdfs://localhost:19000` \n",
    "\n",
    "After creating those two extra environment variables, you can check by calling the following in your CLI:\n",
    "```\n",
    "$ echo %HADOOP_CP%\n",
    "$ echo %HDFS_LOC%\n",
    "```\n",
    "\n",
    "## Step 2 - Compiling our project\n",
    "Run the following commands in your CLI with your respective `.java` files.\n",
    "```\n",
    "$ mkdir dist/\n",
    "$ javac -cp %HADOOP_CP% <some_directory>/*.java -d dist/\n",
    "```\n",
    "\n",
    "## Step 3 - Producing a JAR file\n",
    "Run the following commands to create a JAR file with the compiled classes from Step 2.\n",
    "```\n",
    "$ cd dist\n",
    "$ jar -cvf <application_name>.jar <some_directory>/*.class\n",
    "added manifest\n",
    "...\n",
    "```\n",
    "\n",
    "## Step 4 - Copy our inputs to HDFS\n",
    "Make sure that HDFS and YARN daemons are running. We can now copy our inputs to the HDFS using the `copyFromLocal` command and verify the contents with the `ls` command :\n",
    "```\n",
    "$ hadoop fs -copyFromLocal <some_directory>/input %HDFS_LOC%/input\n",
    "$ hadoop fs -ls %HDFS_LOC%/input\n",
    "Found X items\n",
    "...\n",
    "```\n",
    "\n",
    "## Step 5 - Run our MapReduce Job\n",
    "Run the following commands in the `dist` folder when we originally compiled our code:\n",
    "```\n",
    "$ hadoop jar <application_name>.jar <pkg_name>.<class_name> %HDFS_LOC%/input %HDFS_LOC%/output\n",
    "2020-10-16 17:44:40,331 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
    "...\n",
    "2020-10-16 17:44:43,115 INFO mapreduce.Job: Running job: job_1602881955102_0001\n",
    "2020-10-16 17:44:55,439 INFO mapreduce.Job: Job job_1602881955102_0001 running in uber mode : false\n",
    "2020-10-16 17:44:55,441 INFO mapreduce.Job:  map 0% reduce 0%\n",
    "2020-10-16 17:45:04,685 INFO mapreduce.Job:  map 100% reduce 0%\n",
    "2020-10-16 17:45:11,736 INFO mapreduce.Job:  map 100% reduce 100%\n",
    "2020-10-16 17:45:11,748 INFO mapreduce.Job: Job job_1602881955102_0001 completed successfully\n",
    "...\n",
    "```\n",
    "\n",
    "We can verify the contents of our output by using the `cat` command just like in shell:\n",
    "```\n",
    "$ hadoop fs -cat %HDFS_LOC%/output/part*\n",
    "2020-10-16 18:19:50,225 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
    "...\n",
    "```\n",
    "\n",
    "## Step 6 - Copy outputs to our local machine\n",
    "Once we are satisfied with the results, we can copy the contents to our local machine using the `copyToLocal` command:\n",
    "```\n",
    "$ hadoop fs -copyToLocal %HDFS_LOC%/output <some_output_directory>/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas Individu\n",
    "\n",
    "1. **Verifikasi Instalasi:**\n",
    "   - Buka Command Prompt atau terminal.\n",
    "   - Jalankan perintah berikut untuk memeriksa versi Hadoop yang terinstal:\n",
    "     ```\n",
    "     hadoop version\n",
    "     ```\n",
    "   - Pastikan output menunjukkan versi Hadoop yang benar dan tidak ada pesan error.\n",
    "\n",
    "2. **Mengeksplorasi Hadoop File System (HDFS):**\n",
    "   - Buat direktori baru di HDFS:\n",
    "     ```\n",
    "     hdfs dfs -mkdir /user/test\n",
    "     ```\n",
    "   - Unggah file dari sistem file lokal ke direktori HDFS yang baru dibuat:\n",
    "     ```\n",
    "     hdfs dfs -put localfile.txt /user/test/\n",
    "     ```\n",
    "   - Tampilkan daftar file dan direktori dalam direktori HDFS:\n",
    "     ```\n",
    "     hdfs dfs -ls /user/test/\n",
    "     ```\n",
    "   - Baca isi file yang diunggah ke HDFS:\n",
    "     ```\n",
    "     hdfs dfs -cat /user/test/localfile.txt\n",
    "     ```\n",
    "\n",
    "3. **Menjalankan Contoh MapReduce:**\n",
    "   - Hadoop datang dengan beberapa contoh program MapReduce. Salah satunya adalah program wordcount yang menghitung jumlah kemunculan setiap kata dalam file teks.\n",
    "   - Buat file teks dengan beberapa konten di sistem file lokal.\n",
    "   - Unggah file teks tersebut ke HDFS (gunakan langkah dari bagian sebelumnya).\n",
    "   - Jalankan contoh wordcount MapReduce:\n",
    "     ```\n",
    "     hadoop jar /path/to/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.x.x.jar wordcount /user/test/localfile.txt /user/test/output\n",
    "     ```\n",
    "   - Periksa output dari pekerjaan MapReduce:\n",
    "     ```\n",
    "     hdfs dfs -cat /user/test/output/part-r-00000\n",
    "     ```\n",
    "\n",
    "4. **Konfigurasi dan Penggunaan YARN:**\n",
    "   - Jalankan perintah berikut untuk memulai ResourceManager dan NodeManager YARN:\n",
    "     ```\n",
    "     start-yarn.cmd\n",
    "     ```\n",
    "   - Buka antarmuka web ResourceManager di browser dengan mengunjungi http://localhost:8088. Anda dapat melihat informasi tentang kluster YARN dan pekerjaan yang sedang berjalan atau selesai.\n",
    "\n",
    "5. **Bersihkan Lingkungan:**\n",
    "   - Hapus file dan direktori yang dibuat di HDFS:\n",
    "     ```\n",
    "     hdfs dfs -rm -r /user/test/\n",
    "     ```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
