{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMlaz4EuqgaUMRt5E1jPmzS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from tensorflow import keras\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import GRU,LSTM, Input, Dense, Embedding\n","from tensorflow.keras.callbacks import EarlyStopping\n","from nltk.translate.bleu_score import sentence_bleu"],"metadata":{"id":"zDwmPGKmHPcU","executionInfo":{"status":"ok","timestamp":1731393829633,"user_tz":-420,"elapsed":14729,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"ChkEVMZZHEwt","executionInfo":{"status":"error","timestamp":1731394237786,"user_tz":-420,"elapsed":12738,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}},"outputId":"0f3c42a0-3851-46d6-f86f-5e7df530d899"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Input 0 of layer \"gru_1\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 128)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-8b565513657a>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mencoder_gru2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mencoder_outputs2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_h2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_gru2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mencoder_gru3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_last_axis_squeeze\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    187\u001b[0m                     \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                     \u001b[0;34m\"is incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"gru_1\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 128)"]}],"source":["''' Reading Data '''\n","df = pd.read_csv(\"/content/wiki_movie_plots_deduped - wiki_movie_plots_deduped.csv\")\n","df = df[(df['Origin/Ethnicity'] == 'American') | (df['Origin/Ethnicity'] == 'Chinese') |\n","        (df['Origin/Ethnicity'] == 'British') | (df['Origin/Ethnicity'] == 'Japanese') |\n","        (df['Origin/Ethnicity'] == 'Bollywood')]\n","df_txt = df.loc[:len(df)//2, :]\n","\n","''' Tokenization & Padding '''\n","token = Tokenizer()\n","token.fit_on_texts(df_txt['Plot'])\n","seq = token.texts_to_sequences(df_txt['Plot'])\n","max_len = max([len(x) for x in seq])\n","pad = pad_sequences(seq, maxlen=max_len, padding='post')\n","\n","token2 = Tokenizer()\n","token2.fit_on_texts(df_txt['Title'])\n","seq2 = token2.texts_to_sequences(df_txt['Title'])\n","max_len_2 = max([len(y) for y in seq2])\n","pad2 = pad_sequences(seq2, maxlen=max_len_2, padding='post')\n","\n","voc_size = len(token.word_index) + 1\n","voc_size2 = len(token2.word_index) + 1\n","\n","''' Clear session and Model Definition '''\n","K.clear_session()\n","\n","latent_dim = 128  # Dimension of hidden state\n","embedding_dim = 100  # Coba 50, 100, atau 200 untuk dimensi embedding\n","\n","# Encoder with 3 layers of GRU\n","encoder_inputs = Input(shape=(pad.shape[1],))\n","encoder_embedding = Embedding(voc_size, embedding_dim)(encoder_inputs)\n","encoder_gru1 = GRU(latent_dim, return_state=True)\n","encoder_outputs1, state_h1 = encoder_gru1(encoder_embedding)\n","\n","encoder_gru2 = GRU(latent_dim, return_state=True)\n","encoder_outputs2, state_h2 = encoder_gru2(encoder_outputs1)\n","\n","encoder_gru3 = GRU(latent_dim, return_state=True)\n","encoder_outputs3, state_h3 = encoder_gru3(encoder_outputs2)\n","\n","encoder_states = [state_h3]\n","\n","# Decoder\n","decoder_inputs = Input(shape=(pad2.shape[1],))\n","decoder_embedding = Embedding(voc_size2, embedding_dim)(decoder_inputs)\n","decoder_gru = GRU(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _ = decoder_gru(decoder_embedding, initial_state=encoder_states)\n","\n","decoder_dense = Dense(voc_size2, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","''' Compile Model '''\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","''' EarlyStopping Callback '''\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n","\n","''' Training Model '''\n","history = model.fit([pad, pad2], pad2.reshape(pad2.shape[0], pad2.shape[1], 1), epochs=10, batch_size=16,\n","                    callbacks=[es], validation_split=0.2)\n","\n","''' Plot Training and Validation Loss and Accuracy '''\n","plt.figure(figsize=(12, 5))\n","\n","# Plot Loss\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['loss'], label='Training Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.title('Training and Validation Loss')\n","\n","# Plot Accuracy\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['accuracy'], label='Training Accuracy')\n","plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.title('Training and Validation Accuracy')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","''' Evaluasi Model '''\n","val_loss, val_accuracy = model.evaluate([pad, pad2[::-1]], pad2.reshape(pad2.shape[0], pad2.shape[1], 1), verbose=0)\n","print(f\"Validation Loss: {val_loss:.4f}\")\n","print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n","\n","''' Calculate BLEU Score '''\n","def calculate_bleu(model, x_val, y_val):\n","    bleu_scores = []\n","    for i in range(len(x_val)):\n","        predicted_seq = model.predict(x_val[i:i+1])\n","        predicted_seq = np.argmax(predicted_seq, axis=-1)\n","        reference = [y_val[i].tolist()]\n","        candidate = predicted_seq.flatten().tolist()\n","        bleu_score = sentence_bleu(reference, candidate)\n","        bleu_scores.append(bleu_score)\n","    return np.mean(bleu_scores)\n","\n","bleu_score = calculate_bleu(model, [pad, pad2[::-1]], pad2)\n","print(f\"Validation BLEU Score: {bleu_score:.4f}\")\n"]}]}