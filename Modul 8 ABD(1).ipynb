{"cells":[{"cell_type":"markdown","metadata":{"id":"M-Gd83fOxEl1"},"source":["# Modul 8 Analisis Big Data\n","# Time Series Analysis dengan PySpark"]},{"cell_type":"markdown","metadata":{"id":"5UnZuKIBxEl3"},"source":["#Import useful libraries"]},{"cell_type":"code","source":["pip install pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v5SN-JtgxJuM","executionInfo":{"status":"ok","timestamp":1715653894374,"user_tz":-420,"elapsed":8523,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}},"outputId":"59654dca-7950-4a27-ee84-70f87b51a5a2"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"3XPit8SCxEl4","executionInfo":{"status":"ok","timestamp":1715653707730,"user_tz":-420,"elapsed":1186,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"outputs":[],"source":["import pyspark\n","from pyspark.sql import *\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from datetime import datetime"]},{"cell_type":"markdown","metadata":{"id":"C8XfsobcxEl6"},"source":["#Read dataset from csv\n","\n","**NB**: For some reason yahoo finance responded with 403 when trying to download the dataset directly with wget, so the file \"BTC_USD.csv\" (included in the archive) needs to be added to DBFS at \"dbfs:/FileStore/BTC_USD/BTC_USD.csv\""]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# Create a SparkSession\n","spark = SparkSession.builder \\\n","    .appName(\"Read BTC-USD data\") \\\n","    .getOrCreate()\n","\n","# Read the CSV file into a DataFrame\n","df = spark.read \\\n","    .format(\"csv\") \\\n","    .option(\"header\", \"true\") \\\n","    .option(\"inferSchema\", \"true\") \\\n","    .load(\"/content/BTC-USD.csv\")\n","\n","# Show the DataFrame schema\n","df.printSchema()\n","\n","# Show the first few rows of the DataFrame\n","df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wm7FZ5Gcxndn","executionInfo":{"status":"ok","timestamp":1715653916190,"user_tz":-420,"elapsed":1465,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}},"outputId":"6a453312-6498-439a-fed6-49976904e29f"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- Date: date (nullable = true)\n"," |-- Open: double (nullable = true)\n"," |-- High: double (nullable = true)\n"," |-- Low: double (nullable = true)\n"," |-- Close: double (nullable = true)\n"," |-- Adj Close: double (nullable = true)\n"," |-- Volume: long (nullable = true)\n","\n","+----------+----------+----------+----------+----------+----------+--------+\n","|      Date|      Open|      High|       Low|     Close| Adj Close|  Volume|\n","+----------+----------+----------+----------+----------+----------+--------+\n","|2014-09-17|465.864014|468.174011|452.421997|457.334015|457.334015|21056800|\n","|2014-09-18|456.859985|456.859985|413.104004|424.440002|424.440002|34483200|\n","|2014-09-19|424.102997|427.834991|384.532013| 394.79599| 394.79599|37919700|\n","|2014-09-20|394.673004| 423.29599|389.882996|408.903992|408.903992|36863600|\n","|2014-09-21|408.084991|412.425995|   393.181|398.821014|398.821014|26580100|\n","|2014-09-22|399.100006|406.915985|397.130005|402.152008|402.152008|24127600|\n","|2014-09-23| 402.09201|441.557007|396.196991|435.790985|435.790985|45099500|\n","|2014-09-24|435.751007|   436.112|421.131989|423.204987|423.204987|30627700|\n","|2014-09-25|423.156006|423.519989|409.467987|411.574005|411.574005|26814400|\n","|2014-09-26|411.428986|414.937988|400.009003|404.424988|404.424988|21460800|\n","|2014-09-27|   403.556|406.622986|397.372009|399.519989|399.519989|15029300|\n","|2014-09-28|399.471008|401.016998|374.332001|   377.181|   377.181|23613300|\n","|2014-09-29|376.928009|385.210999| 372.23999| 375.46701| 375.46701|32497700|\n","|2014-09-30|376.088013| 390.97699|373.442993|   386.944|   386.944|34707300|\n","|2014-10-01|387.427002|391.378998|380.779999| 383.61499| 383.61499|26229400|\n","|2014-10-02|383.988007|385.497009|372.946014|375.071991|375.071991|21777700|\n","|2014-10-03|   375.181|377.695007|357.859009|359.511993|359.511993|30901200|\n","|2014-10-04|359.891998|   364.487|325.885986|328.865997|328.865997|47236500|\n","|2014-10-05|328.915985|341.800995| 289.29599| 320.51001| 320.51001|83308096|\n","|2014-10-06|320.389008|345.134003|302.559998| 330.07901| 330.07901|79011800|\n","+----------+----------+----------+----------+----------+----------+--------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"FTA2rPV4xEmJ"},"source":["#Train-test split\n","\n","Validation not needed because CrossValidator will use part of train set as validation set (KFold)\n","\n","Proportion of split is ~ 70/30"]},{"cell_type":"code","source":["# Filter the DataFrame for train and test sets\n","train_set = df.filter(col(\"Date\") < \"2019-09-17\")\n","test_set = df.filter(col(\"Date\") >= \"2019-09-17\")\n","\n","# Show the number of rows in each set\n","print(\"Number of rows in train set:\", train_set.count())\n","print(\"Number of rows in test set:\", test_set.count())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T6qdpyDn6cEm","executionInfo":{"status":"ok","timestamp":1715653925005,"user_tz":-420,"elapsed":503,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}},"outputId":"85c5b307-c2b5-4488-aa1c-ab1a37daa74c"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of rows in train set: 1826\n","Number of rows in test set: 861\n"]}]},{"cell_type":"markdown","metadata":{"id":"q-MGHjiOxEmL"},"source":["#Feature importance analysis\n","\n","Pearson and Spearman correlation matrices used to study the correlation between each couple of features.\n","\n","Being all features quite highly correlated, I choose to keep just one of them (close price) to be able to use a window of more days in the models"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KelerZqwxEmM","executionInfo":{"status":"ok","timestamp":1715653999003,"user_tz":-420,"elapsed":3402,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}},"outputId":"22a7f1bc-ae34-4921-a3d9-ac495701d11f"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[1.         0.99883087 0.99953588 0.99908664 0.73391176]\n"," [0.99883087 1.         0.99948583 0.99940123 0.73284666]\n"," [0.99953588 0.99948583 1.         0.99904118 0.73738097]\n"," [0.99908664 0.99940123 0.99904118 1.         0.72644921]\n"," [0.73391176 0.73284666 0.73738097 0.72644921 1.        ]]\n","\n","[[1.         0.99909409 0.99955709 0.99941132 0.93222606]\n"," [0.99909409 1.         0.99951155 0.99957452 0.93249602]\n"," [0.99955709 0.99951155 1.         0.9991788  0.93351421]\n"," [0.99941132 0.99957452 0.9991788  1.         0.9314874 ]\n"," [0.93222606 0.93249602 0.93351421 0.9314874  1.        ]]\n","\n"]}],"source":["from pyspark.ml.stat import Correlation\n","from pyspark.ml.feature import VectorAssembler\n","\n","def show_matrix(matrix):\n","    \"\"\"\n","    function to print a matrix on screen\n","    \"\"\"\n","    print(matrix.collect()[0][matrix.columns[0]].toArray())\n","    print()\n","\n","vector_col = \"features\"\n","assembler = VectorAssembler(inputCols=[\"Open\", \"Close\", \"High\", \"Low\", \"Volume\"], outputCol=vector_col)\n","df_vector = assembler.transform(df).select(vector_col)\n","\n","matrix_pearson = Correlation.corr(df_vector, vector_col)    #pearson is default\n","matrix_spearman = Correlation.corr(df_vector, vector_col, \"spearman\")\n","show_matrix(matrix_pearson)\n","show_matrix(matrix_spearman)\n"]},{"cell_type":"markdown","metadata":{"id":"ziftpOZgxEmM"},"source":["#Feature scaling\n","\n","Tanh estimator used to scale all the feature values"]},{"cell_type":"code","source":["import math"],"metadata":{"id":"j06r4YlQ6qr0","executionInfo":{"status":"ok","timestamp":1715653746280,"user_tz":-420,"elapsed":21,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"HnotG3LXxEmM","executionInfo":{"status":"error","timestamp":1715654160225,"user_tz":-420,"elapsed":968,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}},"outputId":"2d0b7350-f371-4b25-dceb-69e8232760c8"},"outputs":[{"output_type":"error","ename":"PythonException","evalue":"\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-16-27a285e1caea>\", line 10, in tanh_estimator\nTypeError: unsupported operand type(s) for -: 'float' and 'function'\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-27a285e1caea>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Show the first few rows of scaled DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mscaled_train_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mscaled_test_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m     def _show_string(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-16-27a285e1caea>\", line 10, in tanh_estimator\nTypeError: unsupported operand type(s) for -: 'float' and 'function'\n"]}],"source":["from pyspark.sql.functions import col, udf\n","from pyspark.sql.types import FloatType\n","\n","# Define the Tanh estimator function as a UDF\n","@udf(returnType=FloatType())\n","def tanh_estimator(x):\n","    \"\"\"\n","    User-defined function, applies Tanh estimator's formula to a feature value x\n","    \"\"\"\n","    return 0.5 * (math.tanh(0.01 * (x - mean) / std) + 1)\n","\n","# Define a function to scale the DataFrame using the Tanh estimator function\n","def scale_transform(df):\n","    \"\"\"\n","    Transforms a DataFrame by applying the Tanh estimator UDF\n","    \"\"\"\n","    # Ensure \"Close\" column exists\n","    if \"Close\" not in df.columns:\n","        raise ValueError(\"Column 'Close' not found in DataFrame.\")\n","\n","    # Convert \"Close\" column to FloatType if it's not already\n","    if df.schema[\"Close\"].dataType != FloatType():\n","        df = df.withColumn(\"Close\", col(\"Close\").cast(FloatType()))\n","\n","    # Apply the Tanh estimator UDF to the \"Close\" column\n","    return df.withColumn(\"Scaled_Close\", tanh_estimator(col(\"Close\")))\n","\n","# Apply scaling transformation to the training and test sets\n","scaled_train_set = scale_transform(train_set)\n","scaled_test_set = scale_transform(test_set)\n","\n","# Show the first few rows of scaled DataFrame\n","scaled_train_set.show()\n","scaled_test_set.show()"]},{"cell_type":"markdown","metadata":{"id":"tGFo5fakxEmN"},"source":["#Sliding window\n","\n","Window of 30 days is slided on the close prices in order to create train and test set, composed by examples such as:\n","\n","x={day(i), ... , day(i+29)}, y={day(i+30)}"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"_gV6W8ShxEmN","executionInfo":{"status":"ok","timestamp":1715654340774,"user_tz":-420,"elapsed":509,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"outputs":[],"source":["from pyspark.sql.window import Window\n","\n","def slide_window(df, window_size):\n","    \"\"\"\n","    Returns two new dataframes:\n","    X - obtained sliding a window of given size (=#window_size) on the original dataframe, aggregating #window_size close prices on the same row\n","    y - for each row of X, y contains a row with the (single) price of the day after last day contained in X\n","    \"\"\"\n","\n","    w = Window.orderBy(\"Date\")\n","    indexed_df = df.withColumn(\"Index\", row_number().over(w)).select(\"Index\", \"Close\")    #adding index to be able to loop following order and create windows\n","\n","    schema = StructType([StructField(\"Close\", ArrayType(FloatType()), False)])   #schema for X (array of floats)\n","\n","    X = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema )\n","    y = spark.createDataFrame(spark.sparkContext.emptyRDD(), FloatType())\n","\n","    length = indexed_df.count()\n","    for i in range(window_size+1, length+1):\n","        new_df = indexed_df.where(col(\"Index\").between(i-window_size, i-1)).select(\"Close\")    #select the window\n","        new_row = new_df.agg(collect_list(\"Close\").alias(\"Close\"))    #create new X's row with all prices from window\n","        X = X.union(new_row)\n","        new_row = indexed_df.where(col(\"Index\") == i).select(\"Close\")    #create new Y's row with price of the day after last day contained in X\n","        y = y.union(new_row)\n","\n","    return X, y"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"V2WeVOFfxEmO","executionInfo":{"status":"ok","timestamp":1715657119106,"user_tz":-420,"elapsed":2758623,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"outputs":[],"source":["window = 30    #window size\n","\n","X_train, y_train = slide_window(scaled_train_set, window)    #slide window on train set\n","X_test, y_test = slide_window(scaled_test_set, window)    #slide window on test set"]},{"cell_type":"markdown","metadata":{"id":"xiNPd2klxEmO"},"source":["#Merging X and y\n","\n","X and y (for both train and test) need to be merged as the Pyspark regression models require them in a single dataframe"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"jGrvAZzMxEmO","executionInfo":{"status":"ok","timestamp":1715659791529,"user_tz":-420,"elapsed":590246,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"outputs":[],"source":["def merge_X_y(X, y):\n","    \"\"\"\n","    merges two dataframes column-wise\n","    \"\"\"\n","    schema = StructType(X.schema.fields + y.schema.fields)\n","    X_y = X.rdd.zip(y.rdd).map(lambda x: x[0]+x[1])\n","    return spark.createDataFrame(X_y, schema)\n","\n","X_y_train = merge_X_y(X_train, y_train)\n","X_y_test = merge_X_y(X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"dFkJF3SCxEmP"},"source":["#Vectorization of windows\n","\n","Windows represented as lists of days need to be converted to vectors (rows) of features as Pyspark regression models require dataframes in this form"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zahNgRaexEmP","executionInfo":{"status":"aborted","timestamp":1715653752313,"user_tz":-420,"elapsed":7,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"outputs":[],"source":["from pyspark.ml.linalg import Vectors, VectorUDT\n","from pyspark.sql.functions import udf\n","from pyspark import StorageLevel\n","\n","list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())    #converts list of prices to features vector\n","\n","def assemble_window(X_y):\n","    \"\"\"\n","    applies list_to_vector_udf to given dataframe\n","    \"\"\"\n","    return X_y.select(list_to_vector_udf(X_y[\"Close\"]).alias(\"features\"), X_y[\"value\"].alias(\"label\"))\n","\n","X_y_train_vec = assemble_window(X_y_train)\n","X_y_test_vec = assemble_window(X_y_test)"]},{"cell_type":"markdown","metadata":{"id":"-h_dQA43xEmP"},"source":["#Hyperparameter tuning/model selection/evaluation\n","\n","In this section linear regression and gradient-boosted trees regression models are cross-validated partitioning the train set in 3 folds in order to tune their hyperparameters and find the best model.\n","\n","Then the best models found are tested on unseen data (test set) and the actual and predicted prices are plotted."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Apb1nM0XxEmP","executionInfo":{"status":"aborted","timestamp":1715653752313,"user_tz":-420,"elapsed":7,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"outputs":[],"source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n","from pyspark.ml.evaluation import RegressionEvaluator\n","\n","def cross_validate(model, param_grid, df):\n","    \"\"\"\n","    Performs grid search on given model with given parameter grid, using given dataframe as train/validation data.\n","    Returns the validated model (ready to be used for predictions using best parameters found)\n","    \"\"\"\n","    evaluator = RegressionEvaluator(metricName=\"rmse\")\n","    cv = CrossValidator(estimator=model, estimatorParamMaps=param_grid, evaluator=evaluator)\n","    validated_model = cv.fit(df)\n","    return validated_model"]},{"cell_type":"markdown","metadata":{"id":"R6d2DEMOxEmQ"},"source":["#Linear Regression"]},{"cell_type":"code","source":["print(\"X_train shape:\", X_train.schema)\n","print(\"y_train shape:\", y_train.schema)"],"metadata":{"id":"IoTi3E8P1izX","executionInfo":{"status":"aborted","timestamp":1715653752313,"user_tz":-420,"elapsed":7,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print (X_test, X_train)"],"metadata":{"id":"VS1SujaN0ibl","executionInfo":{"status":"aborted","timestamp":1715653752314,"user_tz":-420,"elapsed":8,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RkfF5ODHxEmQ","executionInfo":{"status":"aborted","timestamp":1715653752314,"user_tz":-420,"elapsed":8,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"outputs":[],"source":["from pyspark.ml.regression import LinearRegression\n","\n","evaluator = RegressionEvaluator(metricName=\"rmse\")\n","\n","lr = LinearRegression(standardization=False)    #avoid standardization as tanh estimator has been applied yet\n","param_grid = ParamGridBuilder().addGrid(lr.regParam, [0.33, 0.66]).addGrid(lr.elasticNetParam, [0.33, 0.5, 0.66]).build()\n","validated_lr_model = cross_validate(lr, param_grid, X_y_train_vec)\n","\n","#best parameters found\n","elasticNet = validated_lr_model.bestModel.getElasticNetParam()\n","reg = validated_lr_model.bestModel.getRegParam()\n","\n","print(\"ElasicNetParam of best model -> \", elasticNet)\n","print(\"RegParam of best model -> = \", reg)\n","\n","predictions = validated_lr_model.transform(X_y_test_vec)    #test on unseen data\n","RMSE = evaluator.evaluate(predictions)    #evaluate predictions using ROOT MEAN SQUARED ERROR\n","print(\"RMSE of best model on unseen data -> \", RMSE)\n","\n","\n","\n","\n","\n","# Membangun dan melatih model regresi linear dengan cross-validation\n","lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\", standardization=False)\n","\n","param_grid = ParamGridBuilder() \\\n","    .addGrid(lr.regParam, [0.33, 0.66]) \\\n","    .addGrid(lr.elasticNetParam, [0.33, 0.5, 0.66]) \\\n","    .build()\n","\n","crossval = CrossValidator(estimator=lr,\n","                          estimatorParamMaps=param_grid,\n","                          evaluator=RegressionEvaluator(metricName=\"rmse\", labelCol=\"label\", predictionCol=\"prediction\"),\n","                          numFolds=3)  # K-Fold Cross Validation\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fmlk3ZZFxEmQ","executionInfo":{"status":"aborted","timestamp":1715653752314,"user_tz":-420,"elapsed":7,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"outputs":[],"source":["def plot_predictions(predictions):\n","    \"\"\"\n","    plots two lines representing predicted and actual prices\n","    \"\"\"\n","    pandas_df = predictions.select('label', 'prediction').toPandas()\n","    plt.figure(figsize=(20, 7))\n","    plt.plot(range(len(pandas_df['label'].values)), pandas_df['label'].values, label = 'Actual Price', color = 'blue')\n","    plt.plot(range(len(pandas_df['prediction'].values)), pandas_df['prediction'].values, label = 'Predicted Price', color = 'red')\n","    plt.xticks(np.arange(100, pandas_df.shape[0], 200))\n","    plt.xlabel('Time')\n","    plt.ylabel('Price (scaled)')\n","    plt.legend()\n","    plt.show()\n","\n","plot_predictions(predictions)    #plot linear regression's predictions"]},{"cell_type":"markdown","metadata":{"id":"eiFFvqRyxEmQ"},"source":["#Gradient-boosted trees"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mvl0vDWUxEmR","executionInfo":{"status":"aborted","timestamp":1715653752314,"user_tz":-420,"elapsed":7,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"outputs":[],"source":["from pyspark.ml.regression import GBTRegressor\n","\n","gbt_r = GBTRegressor()\n","param_grid = ParamGridBuilder().addGrid(gbt_r.maxDepth, [4, 8, 12]).addGrid(gbt_r.featureSubsetStrategy, ['0.33', '0.66']).build()    #parameters to be tuned\n","validated_gbt_model = cross_validate(gbt_r, param_grid, X_y_train_vec)\n","\n","#best parameters found\n","max_depth = validated_gbt_model.bestModel.getMaxDepth()\n","subsample = validated_gbt_model.bestModel.getFeatureSubsetStrategy()\n","\n","print(\"maxDepth of best model -> \", max_depth)\n","print(\"featureSubsetStrategy of best model -> = \", subsample)\n","\n","predictions = validated_gbt_model.transform(X_y_test_vec)    #test on unseen data\n","RMSE = evaluator.evaluate(predictions)    #evaluate predictions using ROOT MEAN SQUARED ERROR\n","print(\"RMSE of best model on unseen data ->  \", RMSE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Bbyz3cVxEmR","executionInfo":{"status":"aborted","timestamp":1715653752315,"user_tz":-420,"elapsed":8,"user":{"displayName":"065_MELIZA WULANDARI","userId":"15457601190170839248"}}},"outputs":[],"source":["plot_predictions(predictions)    #plot gradient-boosted trees' predicitons"]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}